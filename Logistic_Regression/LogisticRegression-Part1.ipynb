{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[1]$ --- Implementable tensor methods in unconstrained convex optimization, https://alfresco.uclouvain.be/alfresco/service/guest/streamDownload/workspace/SpacesStore/aabc2323-0bc1-40d4-9653-1c29971e7bd8/coredp2018_05web.pdf?guest=true.\n",
    "\n",
    "$[2]$ ---  Cubic regularization of Newton method and its global performance, http://lab7.ipu.ru/files/polyak/Nest_Pol-MathProg'06.pdf.\n",
    "\n",
    "$[3]$ --- Acelerating the cubic regularization of Newton’s method on convex\n",
    "problems, http://webdoc.sub.gwdg.de/ebook/serien/e/CORE/dp2005_68.pdf.\n",
    "\n",
    "$[4]$ --- A.R. Conn, N.I. M. Gould, and Ph.L. Toint. Trust Region Methods. SIAM, Philadelphia, 2000.\n",
    "\n",
    "$[5]$ --- Анализ быстрого градиентного метода Нестерова для задач машинного обучения с $L_1$-регуляризацией, http://www.machinelearning.ru/wiki/images/0/03/Rodomanov_FGM.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of code: plot graphs for methods $[1]$.$[2.16]$ and $[1]$.$[3.12]$ with $p = 2$ and $p = 3$ by testing on logistic regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from numpy import linalg, loadtxt\n",
    "import math\n",
    "import autograd.scipy as scipy\n",
    "from scipy import optimize\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation parameters $c, A \\succeq 0, \\gamma > 0$ for auxiliary minimization problem $[1]$.$[5.8]$ in terms of problem \n",
    "$$\n",
    "\\langle c, h \\rangle + \\frac{1}{2}\\langle Ah, h \\rangle + \\frac{\\gamma}{4}||h||_2^4 \\rightarrow \\min\\limits_{h \\in \\mathbb{R}^n}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cur_params(grad_f_cur, hessian_f_cur, tensor3_f_cur_vect, hk, tau, L):\n",
    "    grad_d = np.dot(hk, hk.T)*hk\n",
    "    c = (grad_f_cur - 1/tau*np.dot(hessian_f_cur, hk.T) + 0.5*tensor3_f_cur_vect \n",
    "                                                                        - tau*(tau + 1)/2*L*grad_d) + tau*tau*L/2*grad_d\n",
    "    \n",
    "    A = (tau + 1) / tau * hessian_f_cur\n",
    "    \n",
    "    gamma = tau*(tau + 1)/2*L\n",
    "    return c, A, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function $f_{c, A, \\gamma}(h) = \\langle c, h \\rangle + \\frac{1}{2}\\langle Ah, h \\rangle + \\frac{\\gamma}{4}||h||_2^4$ for auxiliary minimization problem $[1]$.$[5.8]$ and its univariate dual function $g_{c, A, \\gamma}(\\tau) = \\frac{1}{2}\\tau^2 + \\frac{1}{2}\\Bigl\\langle \\bigl(\\sqrt{2\\gamma}\\tau B + A\\bigr)^{-1}c, c \\Bigr\\rangle$ with $B = I_{n \\times n}$ for Euclid 2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_problem_func(h, *args):\n",
    "    c, A, gamma = args\n",
    "    d = linalg.norm(h)\n",
    "    d4 = d*d*d*d\n",
    "    return np.dot(c, h) + 0.5 * np.dot(np.dot(A, h), h) + 0.25*gamma*d4 \n",
    "\n",
    "def aux_problem_onedim_func(tau, *args):\n",
    "    c, A, B, gamma = args\n",
    "    sgam = math.sqrt(2*gamma)\n",
    "    S = sgam*tau*B + A\n",
    "    invS = linalg.inv(S)\n",
    "    f = 0.5*tau*tau + 0.5 * np.dot(np.dot(invS, c), c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation minimum of $f_{c, A, \\gamma}(h)$ using minimum of dual function $g_{c, A, \\gamma}(\\tau)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hopt(t, *args):\n",
    "    c, A, B, gamma = args\n",
    "    S = math.sqrt(2*gamma)*t*B + A\n",
    "    invS = linalg.inv(S)\n",
    "    h_opt = -np.dot(invS, c)\n",
    "    return h_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newton's method for finding minimum $g_{c, A, \\gamma}(\\tau)$ based on $[4]$, Chapter $7$.\n",
    "\n",
    "The main idea is to find root $\\lambda > 0$ of first-order optimality condition: $\\sqrt{\\lambda} = ||s(\\lambda)||_2, s(\\lambda) = -(A + \\lambda I)^{-1}c_{\\text{new}}, \\lambda = \\sqrt{2\\gamma}\\tau, c_{\\text{new}} = \\sqrt{\\gamma}c$.\n",
    "\n",
    "It is better to solve the secular equation:\n",
    "$$\n",
    "\\phi(\\lambda) = \\frac{1}{||s(\\lambda)||_{2}} - \\frac{1}{\\sqrt{\\lambda}}.\n",
    "$$\n",
    "So \n",
    "$$\n",
    "\\phi^{'}(\\lambda) = \\frac{\\bigl \\langle s(\\lambda), A(\\lambda)^{-1}s(\\lambda) \\bigr \\rangle}{||s(\\lambda)||_2^3} + \\frac{1}{2\\lambda^{\\frac{3}{2}}} \\geq 0, A(\\lambda) = A + \\lambda I;\n",
    "$$\n",
    "$$\n",
    "\\phi^{''}(\\lambda) = \\frac{3\\Bigl(\\bigl \\langle s(\\lambda), \\nabla_{\\lambda}s(\\lambda) \\bigr \\rangle^2 - ||s(\\lambda)||_2^2||\\nabla_{\\lambda}s(\\lambda)||_2^2\\Bigr)}{||s(\\lambda)||_2^5} - \\frac{3}{4\\lambda^{\\frac{5}{2}}} < 0\n",
    "$$ by Cauchy-Schwartz inequality.\n",
    "\n",
    "Newton's method for finding a root of the scalar equation $\\phi(\\lambda) = 0$ replaces  the estimate $\\lambda_k > 0$ with the improved estimate $\\lambda_{k + 1}$ for which\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k - \\frac{\\phi(\\lambda_k)}{\\phi^{'}(\\lambda_k)}.\n",
    "$$\n",
    "Using Cholesky factors $A(\\lambda) = L(\\lambda)L^T(\\lambda)$ we can write:\n",
    "$$\n",
    "\\bigl \\langle s, A(\\lambda)^{-1}s \\bigr \\rangle = \\bigl \\langle s, L^{-T}L^{-1}s \\bigr \\rangle = \\bigl \\langle L^{-1}s, L^{-1}s \\bigr \\rangle = ||w(\\lambda)||_2^2\n",
    "$$\n",
    "with $w(\\lambda) = L^{-1}(\\lambda)s(\\lambda)$.\n",
    "The complete Newton algorithm is:\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k \\cdot \\Bigl(1 - 2||s(\\lambda_k)||_2^2\\frac{\\sqrt{\\lambda_k} - ||s(\\lambda_k)||_2}{||s(\\lambda_k)||_2^3 + 2||w(\\lambda_k)||_2^2\\lambda_k^{\\frac{3}{2}}}\\Bigr) \n",
    "$$ while $\\lambda_{k + 1} > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_step_tensor3(AuxMaxIter, *args):\n",
    "    c, H, gamma, eps = args\n",
    "    c_new = c*(gamma**0.5)\n",
    "    lambda_ans = eps\n",
    "    k = 0\n",
    "    while (k < AuxMaxIter):\n",
    "        H_lambda = H + lambda_ans*np.identity(H.shape[0])\n",
    "        try:\n",
    "            L = linalg.cholesky(H_lambda)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        s = linalg.solve(H_lambda, -c_new)\n",
    "        w = linalg.solve(L, s)\n",
    "        lambda_next = lambda_ans*(1 - \n",
    "            2*(lambda_ans**0.5 - linalg.norm(s))*(linalg.norm(s)**2)/\n",
    "                                  (linalg.norm(s)**3 + 2*(linalg.norm(w)**2)*(lambda_ans**(1.5))))\n",
    "        if (lambda_next > 0):\n",
    "            lambda_ans = lambda_next\n",
    "        else:\n",
    "            break\n",
    "        k += 1\n",
    "    #tau_opt2 = 0\n",
    "    h_opt1 = -np.dot(linalg.inv(H + lambda_ans*np.identity(H.shape[0])), c)\n",
    "    #h_opt2 = -np.dot(linalg.pinv(H), c)\n",
    "    #val_1 = np.dot(c, h_opt1) + 1/2*np.dot(np.dot(H, h_opt1), h_opt1) + gamma/4*(linalg.norm(h_opt1)**4)\n",
    "    #val_2 = np.dot(c, h_opt2) + 1/2*np.dot(np.dot(H, h_opt2), h_opt2) + gamma/4*(linalg.norm(h_opt2)**4)\n",
    "    #if (val_1 < val_2):\n",
    "     #   h_opt = h_opt1\n",
    "    #else:\n",
    "      #  h_opt = h_opt2\n",
    "    #return h_opt\n",
    "    return h_opt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration of process $[1].[5.8]$ to solve auxiliary minimization problem $[1]$.$[5.4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T3M(x, aux_prob_method, grad_f_cur, hess_f_cur, tensor3_f_direct, TensorMaxIter, AuxMaxIter, tau, L, eps, *args):\n",
    "    \n",
    "    size = x.shape[0]\n",
    "    hk = np.zeros(size)\n",
    "    B = np.eye(size)\n",
    " \n",
    "    if aux_prob_method == 1:\n",
    "        xb = L*1e10\n",
    "        t0 = 0+eps\n",
    "    \n",
    "    for aux_k in range(1, TensorMaxIter + 1):\n",
    "        tensor3_f_cur_vect = tensor3_f_direct(x, hk, *args)\n",
    "        c, A, gamma = calc_cur_params(grad_f_cur, hess_f_cur, tensor3_f_cur_vect, hk, tau, L)\n",
    "            \n",
    "        if aux_prob_method == 1:\n",
    "            allargs = (c, A, B, gamma)\n",
    "\n",
    "            res = optimize.minimize_scalar(aux_problem_onedim_func, args = allargs, method = 'bounded', bounds=(t0, xb), \n",
    "                                                            options={'maxiter': AuxMaxIter, 'disp': False})\n",
    "            \n",
    "            tau_opt = res.x\n",
    "            \n",
    "            h_opt = calc_hopt(tau_opt, *(c, A, B, gamma))\n",
    "                \n",
    "        elif aux_prob_method == 2:\n",
    "            allargs = (c, A, gamma)\n",
    "        \n",
    "            res = optimize.minimize(aux_problem_func, args = allargs, tol = eps, x0 = hk, method = 'Powell', \n",
    "                                                                        options={'maxiter': AuxMaxIter, 'disp': False})                \n",
    "            h_opt = res.x\n",
    "                \n",
    "        elif aux_prob_method == 3:\n",
    "            h_opt = Newton_step_tensor3(AuxMaxIter, *(c, A, gamma, eps))\n",
    "        hk = h_opt\n",
    "    return (hk + x)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor method $[1]$.$[2.16]$ by Yu. Nesterov (2018), $p = 3$\n",
    "Parameters:  \n",
    "$NumIter$ --- max number of steps in tensor method.\n",
    "\n",
    "$TensorMaxIter$ --- max number of iterations in auxiliary problem solution method $[1]$.$[5.8]$.\n",
    "\n",
    "$AuxMaxIter$ --- max number of iterations to perform step $[1]$.$[5.8]$.\n",
    "\n",
    "$x_0$ --- initial point.\n",
    "\n",
    "$f$ --- objective function oracle.  \n",
    "\n",
    "$\\nabla f$ --- objective gradient of $f$ oracle.\n",
    "\n",
    "$\\nabla^2 f$ --- objective hessian of $f$ oracle.\n",
    "\n",
    "$\\langle \\nabla^3 f(x) h, h \\rangle$ --- objective third directional derivative of $f$ at $x$ along direction $h$ oracle .\n",
    "\n",
    "$\\tau > 1$ - parameter in $[1]$.$[5.5]$.\n",
    "\n",
    "$L_3(f)$ - uniform bound for the Lipschitz constant of third derivative.\n",
    "\n",
    "$\\varepsilon$ - parameter to perform step $[1]$.$[5.8]$, meaning depends of $aux\\_prob\\_method$.\n",
    "\n",
    "$aux\\_prob\\_method$ --- how to solve auxiliary problem (5.4): $1$ --- by means of minimization $g_{c, A, \\gamma}(\\tau)$ using $scipy.optimize$, $2$ --- by means of minimization $f_{c, A, \\gamma}(h)$ using $scipy.optimize$, $3$ --- by means of minimization $g_{c, A, \\gamma}(\\tau)$ using Newton's method.\n",
    "\n",
    "$args$ - arguments for parametrization of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor3_iter(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                                             aux_prob_method, *args):\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    xk = copy.deepcopy(x0)\n",
    "\n",
    "    for k in range(NumIter):\n",
    "        f_xk = f(xk, *args)\n",
    "        grad_f_xk = grad_f(xk, *args)\n",
    "        hessian_f_xk = hess_f(xk, *args)\n",
    "        farr[k] = f_xk\n",
    "        #fgradarr[k] = linalg.norm(grad_f_xk)\n",
    "        xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter,\n",
    "                                                             tau, L, eps, *args) \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    #grad_f_xk = grad_f(xk, *args)\n",
    "    farr[NumIter] = f_xk\n",
    "    #fgradarr[NumIter] = linalg.norm(grad_f_xk)\n",
    "    \n",
    "    return xk, f_xk, farr#, fgradarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prox function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxd(x, p):\n",
    "    return 1. / p * (linalg.norm(x)**p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accelerated tensor method $[1]$.$[3.12]$ by Yu. Nesterov (2018), $p = 3$\n",
    "\n",
    "This method has additional parameter $version$ = 1 if we use formula for $A_k$ (as is $[1]$.$[3.11]$):\n",
    "$$\n",
    "A_k = \\Bigl[\\frac{(p - 1)(M^2 - L_p^2)}{4(p + 1)M^2}\\Bigr]^{\\frac{p}{2}}\\Bigl(\\frac{k}{p + 1}\\Bigr)^{p + 1}\n",
    "$$\n",
    "and $version$ = 2 if we use formula for $A_k$:\n",
    "$$\n",
    "A_k = \\Bigl[\\frac{(p + 1)(M^2 - L_p^2)}{4(p - 1)M^2}\\Bigr]^{\\frac{p}{2}}\\Bigl(\\frac{k}{p + 1}\\Bigr)^{p + 1}.\n",
    "$$\n",
    "Both versions use formula $[1]$.$[5.5]$:\n",
    "$$\n",
    "M = \\tau^2 L_3.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorAcc3_iter(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                               aux_prob_method, version, *args):\n",
    "    \n",
    "    xk = copy.deepcopy(x0)\n",
    "    M = tau*tau*L\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    farr[0] = f_xk\n",
    "    grad_f_xk = grad_f(xk, *args)\n",
    "    #fgradarr[0] = linalg.norm(grad_f_xk)\n",
    "    hessian_f_xk = hess_f(xk, *args)\n",
    "  \n",
    "    xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, tau, L, \n",
    "                                                                                                         eps, *args)  \n",
    "    #print('xk = {0}'.format(xk))\n",
    "    f_xk = f(xk, *args)\n",
    "    #gradf_xk = grad_f(xk, *args)\n",
    "    farr[1] = f_xk\n",
    "    #fgradarr[1] = linalg.norm(gradf_xk)\n",
    "    \n",
    "    p = 3\n",
    "    C = p / 2 * math.sqrt((p+1)/(p-1)*(M*M - L*L))\n",
    "    \n",
    "    min_psik = copy.deepcopy(x0)\n",
    "    size = x0.shape[0]\n",
    "    sk = np.zeros(size)\n",
    "    if (version == 1):\n",
    "        ak_part = math.sqrt(pow((p - 1) * (M*M - L*L) / 4 / (p + 1) / M / M, p))\n",
    "    elif (version == 2):\n",
    "        ak_part = math.sqrt(pow((p + 1) * (M*M - L*L) / 4 / (p - 1) / M / M, p))\n",
    "    k = 1\n",
    "    Ak1 = ak_part*pow(k / (p + 1), p + 1)\n",
    "    \n",
    "    factor_p = math.factorial(p)\n",
    "    \n",
    "    for k in range(1, NumIter):\n",
    "        vk = min_psik\n",
    "        Ak = Ak1\n",
    "        Ak1 = ak_part*pow((k + 1) / (p + 1), p + 1)\n",
    "        alpha = Ak / Ak1\n",
    "        #print('alpha = {0}'.format(alpha))\n",
    "        yk = alpha * xk + (1 - alpha) * vk\n",
    "        #print('yk = {0}'.format(yk))\n",
    "        grad_f_yk = grad_f(yk, *args)\n",
    "        hessian_f_yk = hess_f(yk, *args)\n",
    "\n",
    "        xk = T3M(yk, aux_prob_method, grad_f_yk, hessian_f_yk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, \n",
    "                                                                                     tau, L, eps, *args)        \n",
    "        #print('xk = {0}'.format(xk))\n",
    "        grad_next = grad_f(xk, *args)\n",
    "        a = Ak1 - Ak\n",
    "        sk = sk + a * grad_next \n",
    "        min_psik = x0 - pow(factor_p / C / pow(linalg.norm(sk), p - 1), 1. / p) * sk\n",
    "        \n",
    "        f_xk = f(xk, *args)\n",
    "        farr[k + 1] = f_xk\n",
    "        #fgradarr[k + 1] = linalg.norm(grad_next)\n",
    "    \n",
    "    return xk, f_xk, farr#, fgradarr, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accelerated tensor method $[1]$.$[3.12]$ by Yu. Nesterov (2018) with printing history, $p = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorAcc3_iter_hist(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                               aux_prob_method, version, *args):\n",
    "    \n",
    "    xk = copy.deepcopy(x0)\n",
    "    M = tau*tau*L\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    print(\"iter = {0}, f_k = {1}\".format(0, f_xk))\n",
    "    farr[0] = f_xk\n",
    "    grad_f_xk = grad_f(xk, *args)\n",
    "    #fgradarr[0] = linalg.norm(grad_f_xk)\n",
    "    hessian_f_xk = hess_f(xk, *args)\n",
    "  \n",
    "    xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, tau, L, \n",
    "                                                                                                         eps, *args)  \n",
    "    #print('xk = {0}'.format(xk))\n",
    "    f_xk = f(xk, *args)\n",
    "    print(\"iter = {0}, f_k = {1}\".format(1, f_xk))\n",
    "    #gradf_xk = grad_f(xk, *args)\n",
    "    farr[1] = f_xk\n",
    "    #fgradarr[1] = linalg.norm(gradf_xk)\n",
    "    \n",
    "    p = 3\n",
    "    C = p / 2 * math.sqrt((p+1)/(p-1)*(M*M - L*L))\n",
    "    \n",
    "    min_psik = copy.deepcopy(x0)\n",
    "    size = x0.shape[0]\n",
    "    sk = np.zeros(size)\n",
    "    if (version == 1):\n",
    "        ak_part = math.sqrt(pow((p - 1) * (M*M - L*L) / 4 / (p + 1) / M / M, p))\n",
    "    elif (version == 2):\n",
    "        ak_part = math.sqrt(pow((p + 1) * (M*M - L*L) / 4 / (p - 1) / M / M, p))\n",
    "    k = 1\n",
    "    Ak1 = ak_part*pow(k / (p + 1), p + 1)\n",
    "    \n",
    "    factor_p = math.factorial(p)\n",
    "    \n",
    "    for k in range(1, NumIter):\n",
    "        vk = min_psik\n",
    "        Ak = Ak1\n",
    "        Ak1 = ak_part*pow((k + 1) / (p + 1), p + 1)\n",
    "        alpha = Ak / Ak1\n",
    "        #print('alpha = {0}'.format(alpha))\n",
    "        yk = alpha * xk + (1 - alpha) * vk\n",
    "        #print('yk = {0}'.format(yk))\n",
    "        grad_f_yk = grad_f(yk, *args)\n",
    "        hessian_f_yk = hess_f(yk, *args)\n",
    "\n",
    "        xk = T3M(yk, aux_prob_method, grad_f_yk, hessian_f_yk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, \n",
    "                                                                                     tau, L, eps, *args)        \n",
    "        #print('xk = {0}'.format(xk))\n",
    "        grad_next = grad_f(xk, *args)\n",
    "        a = Ak1 - Ak\n",
    "        sk = sk + a * grad_next \n",
    "        min_psik = x0 - pow(factor_p / C / pow(linalg.norm(sk), p - 1), 1. / p) * sk\n",
    "        \n",
    "        f_xk = f(xk, *args)\n",
    "        print(\"iter = {0}, f_k = {1}\".format(k + 1, f_xk))\n",
    "        farr[k + 1] = f_xk\n",
    "        #fgradarr[k + 1] = linalg.norm(grad_next)\n",
    "    \n",
    "    return xk, farr#, fgradarr, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newton's method for finding minimum $\\Omega_{g, H, M}(h) = \\langle g, h \\rangle + \\frac{1}{2}\\langle Hh, h \\rangle + \\frac{M}{6}||h||_2^3$ in $[3]$.$[5.1]$ based on one-dimensional equation $[3]$.$[5.4]$ and methods in $[4]$, Chapter $7$. Note that we assume $M > 0, H \\succeq 0$.\n",
    "\n",
    "The main idea is to find root $ \\lambda_{\\text{root}} > 0$ of first-order optimality condition: $\\lambda = ||s(\\lambda)||_2, s(\\lambda) = -(H + \\lambda I)^{-1}g_{\\text{new}}, \\lambda = \\frac{M}{2}||h_{\\text{opt}}||_2, g_{\\text{new}} = \\frac{M}{2}g, h = -(H + \\lambda_{\\text{root}} I)^{-1}g$.\n",
    "\n",
    "It is better to solve the secular equation:\n",
    "$$\n",
    "\\phi(\\lambda) = \\frac{1}{||s(\\lambda)||_{2}} - \\frac{1}{\\lambda}.\n",
    "$$\n",
    "So \n",
    "$$\n",
    "\\phi^{'}(\\lambda) = \\frac{\\bigl \\langle s(\\lambda), A(\\lambda)^{-1}s(\\lambda) \\bigr \\rangle}{||s(\\lambda)||_2^3} + \\frac{1}{\\lambda^2} \\geq 0, A(\\lambda) = A + \\lambda I;\n",
    "$$\n",
    "$$\n",
    "\\phi^{''}(\\lambda) = \\frac{3\\Bigl(\\bigl \\langle s(\\lambda), \\nabla_{\\lambda}s(\\lambda) \\bigr \\rangle^2 - ||s(\\lambda)||_2^2||\\nabla_{\\lambda}s(\\lambda)||_2^2\\Bigr)}{||s(\\lambda)||_2^5} - \\frac{2}{\\lambda^3} < 0\n",
    "$$ by Cauchy-Schwartz inequality.\n",
    "\n",
    "Newton's method for finding a root of the scalar equation $\\phi(\\lambda) = 0$ replaces  the estimate $\\lambda_k > 0$ with the improved estimate $\\lambda_{k + 1}$ for which\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k - \\frac{\\phi(\\lambda_k)}{\\phi^{'}(\\lambda_k)}.\n",
    "$$\n",
    "Using Cholesky factors $A(\\lambda) = L(\\lambda)L^T(\\lambda)$ we can write:\n",
    "$$\n",
    "\\bigl \\langle s, A(\\lambda)^{-1}s \\bigr \\rangle = \\bigl \\langle s, L^{-T}L^{-1}s \\bigr \\rangle = \\bigl \\langle L^{-1}s, L^{-1}s \\bigr \\rangle = ||w(\\lambda)||_2^2\n",
    "$$\n",
    "with $w(\\lambda) = L^{-1}(\\lambda)s(\\lambda)$.\n",
    "The complete Newton algorithm is:\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k \\cdot \\Bigl(1 - ||s(\\lambda_k)||_2^2\\frac{\\lambda_k - ||s(\\lambda_k)||_2}{||s(\\lambda_k)||_2^3 + ||w(\\lambda_k)||_2^2\\lambda_k^{2}}\\Bigr) \n",
    "$$ while $\\lambda_{k + 1} > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_cubic_step(x_k, NumIterStep, *args):\n",
    "    g, H, M, eps = args\n",
    "    g_new = g*M/2\n",
    "    lambda_ans = eps\n",
    "    k = 0\n",
    "    while (k < NumIterStep):\n",
    "        H_lambda = H + lambda_ans*np.identity(H.shape[0])\n",
    "        try:\n",
    "            L = linalg.cholesky(H_lambda)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        s = linalg.solve(H_lambda, -g_new)\n",
    "        w = linalg.solve(L, s)\n",
    "        lambda_next = lambda_ans*(1 - \n",
    "            (lambda_ans - linalg.norm(s))*(linalg.norm(s)**2)/(linalg.norm(s)**3 + (linalg.norm(w)**2)*(lambda_ans**2)))\n",
    "        if (lambda_next > 0):\n",
    "            lambda_ans = lambda_next\n",
    "        else:\n",
    "            break\n",
    "        k += 1\n",
    "    #r_opt2 = 0\n",
    "    h_opt1 = -np.dot(linalg.inv(H + lambda_ans*np.identity(H.shape[0])), g)\n",
    "    h_opt = h_opt1\n",
    "    #h_opt2 = -np.dot(linalg.pinv(H), g)\n",
    "    #val_1 = np.dot(g, h_opt1) + 1/2*np.dot(np.dot(H, h_opt1), h_opt1) + M/6*(linalg.norm(h_opt1)**3)\n",
    "    #val_2 = np.dot(g, h_opt2) + 1/2*np.dot(np.dot(H, h_opt2), h_opt2) + M/6*(linalg.norm(h_opt2)**3)\n",
    "    #if (val_1 < val_2):\n",
    "    #    h_opt = h_opt1\n",
    "    #else:\n",
    "    #    h_opt = h_opt2\n",
    "    return x_k + h_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function $v_{u}(h) = \\langle g, h \\rangle + \\frac{1}{2}\\langle Hh, h \\rangle + \\frac{M}{6}||h||_2^3$ for auxiliary minimization problem $[1]$.$[2.6]$ with $p = 2$ and its univariate dual function $-v_{l}(r) = \\frac{M}{12}r^3 + \\frac{1}{2}\\Bigl\\langle \\bigl(H + \\frac{Mr}{2}B\\bigr)^{-1}g, g \\Bigr\\rangle$ with $B = I_{n \\times n}$ for Euclid 2-norm (see $[2]$.$[5.2]$). Note that we assume $M > 0, H \\succeq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_problem_func_T2M(h, *args):\n",
    "    g, H, M = args\n",
    "    d = linalg.norm(h)\n",
    "    d3 = d*d*d\n",
    "    return np.dot(g, h) + 0.5 * np.dot(np.dot(H, h), h) + M/6*d3 \n",
    "\n",
    "def aux_problem_onedim_func_T2M(r, *args):\n",
    "    g, H, B, M = args\n",
    "    sr = M*r/2\n",
    "    S = sr*B + H\n",
    "    invS = linalg.inv(S)\n",
    "    f = M*r*r*r/12 + 0.5 * np.dot(np.dot(invS, g), g)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation minimum of $v_{u}(h)$ using minimum of dual function $-v_{l}(r)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hopt_T2M(r, *args):\n",
    "    g, H, B, M = args\n",
    "    S = M/2*r*B + H\n",
    "    invS = linalg.inv(S)\n",
    "    h_opt = -np.dot(invS, g)\n",
    "    return h_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration of process $[1].[2.16]$ to solve auxiliary minimization problem $[1]$.$[2.6], p = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T2M(x, aux_prob_method, grad_f_cur, hess_f_cur, AuxMaxIter, tau, L2, eps, *args):\n",
    "    M = 2*L2*(tau**2)\n",
    "    n = x.shape[0]\n",
    "    hk = np.zeros(n)\n",
    "    B = np.eye(n)\n",
    "    if (aux_prob_method == 1):\n",
    "        xb = L2*1e10\n",
    "        t0 = 0+eps\n",
    "        allargs = (grad_f_cur, hess_f_cur, B, M)\n",
    "\n",
    "        res = optimize.minimize_scalar(aux_problem_onedim_func_T2M, args = allargs, method = 'bounded',\n",
    "                                        bounds=(t0, xb), options={'xatol': eps, 'maxiter': AuxMaxIter, 'disp': False})\n",
    "            \n",
    "        r_opt = res.x\n",
    "            \n",
    "        h_opt = calc_hopt_T2M(r_opt, *(grad_f_cur, hess_f_cur, B, M))  \n",
    "            \n",
    "    if (aux_prob_method == 2):\n",
    "        allargs = (grad_f_cur, hess_f_cur, M)\n",
    "        \n",
    "        res = optimize.minimize(aux_problem_func_T2M, args = allargs, x0 = hk, method = 'Powell', \n",
    "                                                options={'ftol': eps,'maxiter': AuxMaxIter, 'disp': False})                \n",
    "        h_opt = res.x\n",
    "            \n",
    "    if (aux_prob_method == 3):\n",
    "        h_opt = Newton_cubic_step(hk, AuxMaxIter, *(grad_f_cur, hess_f_cur, M, eps))\n",
    "    \n",
    "    return x + h_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor method $[1]$.$[2.16]$ by Yu. Nesterov (2018) with printing history, $p = 2$\n",
    "With $p = 2$ we can write auxiliary problem $[1]$.$[2.6]$ as:\n",
    "$$\n",
    "\\Omega_{x, 2, L_2}(y) = f(x) + \\bigl \\langle \\nabla f(x), y - x \\bigr \\rangle + \\frac{1}{2}\\bigl \\langle \\nabla^2 f(x)(y - x), y - x \\bigr \\rangle + \\frac{\\tau^2L_2}{3}||y - x||_2^3 \\rightarrow \\min\\limits_{y \\in \\mathbb{R}^n}.\n",
    "$$\n",
    "We used formula for $M = \\tau^2L_2$ in $[1]$.$[2.16]$ as in $[1]$.$[5.5]$.\n",
    "\n",
    "So we can solve this problem using Cubic regularization of Newton method according to work $[2]$, problem $[2]$.$[5.1]$ with $M = 2\\tau^2L_2$ and $h = y - x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:  \n",
    "$NumIter$ --- max number of steps in tensor method.\n",
    "\n",
    "$AuxMaxIter$ --- max number of iterations to perform step $[1]$.$[2.16]$.\n",
    "\n",
    "$x_0$ --- initial point.\n",
    "\n",
    "$f$ --- objective function oracle.  \n",
    "\n",
    "$\\nabla f$ --- objective gradient of $f$ oracle.\n",
    "\n",
    "$\\nabla^2 f$ --- objective hessian of $f$ oracle. \n",
    "\n",
    "$\\tau > 1$ - parameter in formula $M = \\tau^2L_2$.\n",
    "\n",
    "$L_2(f)$ - uniform bound for the Lipschitz constant of hessian.\n",
    "\n",
    "$\\varepsilon$ - parameter to perform step $[1]$.$[2.16]$, meaning depends of $aux\\_prob\\_method$.\n",
    "\n",
    "$aux\\_prob\\_method$ --- how to solve auxiliary problem $[1]$.$[2.6]$: $1$ --- by means of minimization $-v_l(r)$ in $[2]$.$[5.2]$ using $scipy.optimize$, $2$ --- by means of minimization $\\Omega_{x, 2, L_2}(y)$ using $scipy.optimize$, $3$ --- by means of minimization $-v_l(r)$ in $[2]$.$[5.2]$ using Newton's method.\n",
    "\n",
    "$args$ - arguments for parametrization of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor2_iter_hist(NumIter, AuxMaxIter, x0, f, grad_f, hess_f, tau, L2, eps, aux_prob_method, *args):\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    xk = copy.deepcopy(x0)\n",
    "\n",
    "    for k in range(NumIter):\n",
    "        f_xk = f(xk, *args)\n",
    "        grad_f_cur = grad_f(xk, *args)\n",
    "        hess_f_cur = hess_f(xk, *args)\n",
    "        farr[k] = f_xk\n",
    "        print('iter = {0}, f_k = {1}'.format(k, f_xk))\n",
    "        #fgradarr[k] = linalg.norm(grad_f_xk)\n",
    "        xk = T2M(xk, aux_prob_method, grad_f_cur, hess_f_cur, AuxMaxIter, tau, L2, eps, *args)\n",
    "        \n",
    "    f_xk = f(xk, *args)\n",
    "    #grad_f_xk = grad_f(xk, *args)\n",
    "    farr[NumIter] = f_xk\n",
    "    print('iter = {0}, f_k = {1}'.format(NumIter, f_xk))\n",
    "    #fgradarr[NumIter] = linalg.norm(grad_f_xk)\n",
    "    \n",
    "    return xk, farr#, fgradarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accelerated tensor method $[1]$.$[3.12]$ by Yu. Nesterov (2018) with printing history, $p = 2$\n",
    "This method has additional parameter $version$ = 1 if we use formula for $A_k$ (as is $[1]$.$[3.11]$):\n",
    "$$\n",
    "A_k = \\Bigl[\\frac{(p - 1)(M^2 - L_p^2)}{4(p + 1)M^2}\\Bigr]^{\\frac{p}{2}}\\Bigl(\\frac{k}{p + 1}\\Bigr)^{p + 1}\n",
    "$$\n",
    "and $version$ = 2 if we use formula for $A_k$:\n",
    "$$\n",
    "A_k = \\Bigl[\\frac{(p + 1)(M^2 - L_p^2)}{4(p - 1)M^2}\\Bigr]^{\\frac{p}{2}}\\Bigl(\\frac{k}{p + 1}\\Bigr)^{p + 1}.\n",
    "$$\n",
    "Both versions use formula as in $[1]$.$[5.5]$:\n",
    "$$\n",
    "M = \\tau^2 L_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorAcc2_iter_hist(NumIter, AuxMaxIter, x0, f, grad_f, hess_f, tau, L2, eps, aux_prob_method, version, *args):\n",
    "    \n",
    "    xk = copy.deepcopy(x0)\n",
    "    M = tau*tau*L2\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    farr[0] = f_xk\n",
    "    print('iter = {0}, f_k = {1}'.format(0, f_xk))\n",
    "    grad_f_xk = grad_f(xk, *args)\n",
    "    #fgradarr[0] = linalg.norm(grad_f_xk)\n",
    "    hessian_f_xk = hess_f(xk, *args)\n",
    "  \n",
    "    xk = T2M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, AuxMaxIter, tau, L2, eps, *args)  \n",
    "    #print('xk = {0}'.format(xk))\n",
    "    f_xk = f(xk, *args)\n",
    "    #gradf_xk = grad_f(xk, *args)\n",
    "    farr[1] = f_xk\n",
    "    #fgradarr[1] = linalg.norm(gradf_xk)\n",
    "    print('iter = {0}, f_k = {1}'.format(1, f_xk))\n",
    "    p = 2\n",
    "    C = p / 2 * math.sqrt((p + 1)/(p - 1)*(M*M - L2*L2))\n",
    "    \n",
    "    min_psik = copy.deepcopy(x0)\n",
    "    n = x0.shape[0]\n",
    "    sk = np.zeros(n)\n",
    "    if (version == 1):\n",
    "        ak_part = math.sqrt(pow((p - 1) * (M*M - L2*L2) / 4 / (p + 1) / M / M, p))\n",
    "    elif (version == 2):\n",
    "        ak_part = math.sqrt(pow((p + 1) * (M*M - L2*L2) / 4 / (p - 1) / M / M, p))\n",
    "    k = 1\n",
    "    Ak1 = ak_part*pow(k / (p + 1), p + 1)\n",
    "    \n",
    "    factor_p = math.factorial(p)\n",
    "    \n",
    "    for k in range(1, NumIter):\n",
    "        vk = min_psik\n",
    "        Ak = Ak1\n",
    "        Ak1 = ak_part*pow((k + 1) / (p + 1), p + 1)\n",
    "        alpha = Ak / Ak1\n",
    "        #print('alpha = {0}'.format(alpha))\n",
    "        yk = alpha * xk + (1 - alpha) * vk\n",
    "        #print('yk = {0}'.format(yk))\n",
    "        grad_f_yk = grad_f(yk, *args)\n",
    "        hessian_f_yk = hess_f(yk, *args)\n",
    "\n",
    "        xk = T2M(yk, aux_prob_method, grad_f_yk, hessian_f_yk, AuxMaxIter, tau, L2, eps, *args)        \n",
    "        #print('xk = {0}'.format(xk))\n",
    "        grad_next = grad_f(xk, *args)\n",
    "        a = Ak1 - Ak\n",
    "        sk = sk + a * grad_next \n",
    "        min_psik = x0 - pow(factor_p / C / pow(linalg.norm(sk), p - 1), 1. / p) * sk\n",
    "        \n",
    "        f_xk = f(xk, *args)\n",
    "        print('iter = {0}, f_k = {1}'.format(k + 1, f_xk))\n",
    "        farr[k + 1] = f_xk\n",
    "        #fgradarr[k + 1] = linalg.norm(grad_next)\n",
    "    \n",
    "    return xk, farr#, fgradarr, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was read successfully!\n"
     ]
    }
   ],
   "source": [
    "features = 54\n",
    "train_object_size = 20000\n",
    "test_object_size = 30000\n",
    "read_object_size = train_object_size + test_object_size\n",
    "\n",
    "train_object = np.zeros((train_object_size, features))\n",
    "train_ans = np.zeros(train_object_size)\n",
    "\n",
    "test_object = np.zeros((test_object_size, features)) \n",
    "test_ans = np.zeros(test_object_size)\n",
    "\n",
    "f = open('covtype.libsvm.binary.scale')\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    if (line_num == read_object_size):\n",
    "        break\n",
    "    line_object = line.split()\n",
    "    len_line_object = len(line_object)\n",
    "    for i in range(1, len_line_object):\n",
    "        current_cell = line_object[i].split(':')\n",
    "        current_num_feature = int(current_cell[0]) - 1\n",
    "        current_feature = float(current_cell[1])\n",
    "        bin_class = int(line_object[0])\n",
    "        if (bin_class == 2):\n",
    "            bin_class = -1\n",
    "        if (line_num < train_object_size):\n",
    "            train_ans[line_num] = bin_class\n",
    "            train_object[line_num][current_num_feature] = current_feature\n",
    "        else:\n",
    "            test_ans[line_num - train_object_size] = bin_class\n",
    "            test_object[line_num - train_object_size][current_num_feature] = current_feature\n",
    "    line_num += 1\n",
    "f.close()\n",
    "print(\"Data was read successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logististic loss and it's gradient, hessian and third derivative using closed-form expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    return sum([np.log(1 + np.exp(-y[i]*np.dot(X[i], w))) for i in range(objects_size)])\n",
    "\n",
    "def grad_logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    return sum([-y[i]*X[i]/(1 + np.exp(y[i]*np.dot(X[i], w))) for i in range(objects_size)])\n",
    "\n",
    "def hess_logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    features = X.shape[1]\n",
    "    ans = np.zeros((features, features))\n",
    "    for i in range(objects_size):\n",
    "        ans += np.exp(y[i]*np.dot(X[i], w))/((1 + np.exp(y[i]*np.dot(X[i], w)))**2)*np.dot(X[i].reshape(-1, 1),\n",
    "                                                                X[i].reshape(1, -1))\n",
    "    return ans\n",
    "\n",
    "def dot_tensor3_and_vector_vector_logistic_loss(w, h, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    features = X.shape[1]\n",
    "    ans = np.zeros(features)\n",
    "    for i in range(objects_size):\n",
    "        add = y[i]*(1 - np.exp(y[i]*np.dot(X[i], w)))*(np.dot(h, X[i])**2)*np.exp(y[i]*np.dot(X[i], w))\n",
    "        add = add/((1 + np.exp(y[i]*np.dot(X[i], w)))**3)*X[i]\n",
    "        ans += add\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upper bounds on $L_2(\\text{logistic loss})$ and $L_3(\\text{logistic loss})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_upper_bound(X):\n",
    "    return 1/10*sum([linalg.norm(X[i])**3 for i in range(X.shape[0])])\n",
    "\n",
    "def L3_upper_bound(X):\n",
    "    return 1/8*sum([linalg.norm(X[i])**4 for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our problem to solve and it's parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_const_feature = np.array([np.array([1]) for i in range(train_object_size)]) #добавим фиктивный признак\n",
    "train_extended_object = copy.deepcopy(train_object)\n",
    "train_extended_object = np.column_stack((train_extended_object, train_const_feature[:, 0]))\n",
    "args = (train_extended_object, train_ans)\n",
    "\n",
    "features_num = features + 1\n",
    "w0 = np.zeros(features_num)\n",
    "\n",
    "NumIter = 100\n",
    "TensorMaxIter = 40\n",
    "AuxMaxIter = 100\n",
    "eps = 1e-7\n",
    "tau = 1 + eps\n",
    "L2 = L2_upper_bound(train_extended_object)\n",
    "L3 = L3_upper_bound(train_extended_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28193.786578212414"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85588.5309823477"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.368684 ,  0.141667 ,  0.0454545,  0.184681 ,  0.223514 ,\n",
       "        0.0716594,  0.870079 ,  0.913386 ,  0.582677 ,  0.875366 ,\n",
       "        1.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  1.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  1.       ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extended_object[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor method $[1]$.$[2.16]$, $p = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, f_k = 13862.943611201723\n",
      "iter = 1, f_k = 11672.897224633785\n",
      "iter = 2, f_k = 10627.36123260829\n",
      "iter = 3, f_k = 9943.21745743802\n",
      "iter = 4, f_k = 9457.699140873674\n",
      "iter = 5, f_k = 9106.554737145934\n",
      "iter = 6, f_k = 8848.53801394512\n",
      "iter = 7, f_k = 8654.464476396899\n",
      "iter = 8, f_k = 8503.922397120326\n",
      "iter = 9, f_k = 8383.00314383373\n",
      "iter = 10, f_k = 8282.468363742362\n",
      "iter = 11, f_k = 8196.305824426463\n",
      "iter = 12, f_k = 8120.647350409391\n",
      "iter = 13, f_k = 8052.99897860391\n",
      "iter = 14, f_k = 7991.7264577297865\n",
      "iter = 15, f_k = 7935.725389956013\n",
      "iter = 16, f_k = 7884.217240778297\n",
      "iter = 17, f_k = 7836.626055612531\n",
      "iter = 18, f_k = 7792.50489421339\n",
      "iter = 19, f_k = 7751.492070211397\n",
      "iter = 20, f_k = 7713.28489152733\n",
      "iter = 21, f_k = 7677.62382663804\n",
      "iter = 22, f_k = 7644.28224529432\n",
      "iter = 23, f_k = 7613.059839382976\n",
      "iter = 24, f_k = 7583.778092456457\n",
      "iter = 25, f_k = 7556.27691071277\n",
      "iter = 26, f_k = 7530.412066133159\n",
      "iter = 27, f_k = 7506.053164535053\n",
      "iter = 28, f_k = 7483.081983843918\n",
      "iter = 29, f_k = 7461.391086887023\n",
      "iter = 30, f_k = 7440.882646118158\n",
      "iter = 31, f_k = 7421.467436840327\n",
      "iter = 32, f_k = 7403.063967135931\n",
      "iter = 33, f_k = 7385.597720255873\n",
      "iter = 34, f_k = 7369.000490436332\n",
      "iter = 35, f_k = 7353.2097969374845\n",
      "iter = 36, f_k = 7338.168364027096\n",
      "iter = 37, f_k = 7323.823656932461\n",
      "iter = 38, f_k = 7310.127465628521\n",
      "iter = 39, f_k = 7297.035529808875\n",
      "iter = 40, f_k = 7284.507199584847\n",
      "iter = 41, f_k = 7272.505127423745\n",
      "iter = 42, f_k = 7260.9949876196415\n",
      "iter = 43, f_k = 7249.94522021456\n",
      "iter = 44, f_k = 7239.326796794686\n",
      "iter = 45, f_k = 7229.113005989488\n",
      "iter = 46, f_k = 7219.279256825312\n",
      "iter = 47, f_k = 7209.802898341721\n",
      "iter = 48, f_k = 7200.663054087494\n",
      "iter = 49, f_k = 7191.840470276787\n",
      "iter = 50, f_k = 7183.317376519631\n",
      "iter = 51, f_k = 7175.077358149021\n",
      "iter = 52, f_k = 7167.1052392545635\n",
      "iter = 53, f_k = 7159.386975607297\n",
      "iter = 54, f_k = 7151.909556722029\n",
      "iter = 55, f_k = 7144.660916357548\n",
      "iter = 56, f_k = 7137.629850803311\n",
      "iter = 57, f_k = 7130.805944343135\n",
      "iter = 58, f_k = 7124.179501327599\n",
      "iter = 59, f_k = 7117.741484321676\n",
      "iter = 60, f_k = 7111.483457830285\n",
      "iter = 61, f_k = 7105.3975371373135\n",
      "iter = 62, f_k = 7099.476341823589\n",
      "iter = 63, f_k = 7093.717083029456\n",
      "iter = 64, f_k = 7088.1146617725935\n",
      "iter = 65, f_k = 7082.662871205517\n",
      "iter = 66, f_k = 7077.355854962667\n",
      "iter = 67, f_k = 7072.188079042498\n",
      "iter = 68, f_k = 7067.154306384295\n",
      "iter = 69, f_k = 7062.249573883211\n",
      "iter = 70, f_k = 7057.469171610668\n",
      "iter = 71, f_k = 7052.808624023925\n",
      "iter = 72, f_k = 7048.208692283596\n",
      "iter = 73, f_k = 7043.716085444488\n",
      "iter = 74, f_k = 7039.327073199267\n",
      "iter = 75, f_k = 7035.038103684336\n",
      "iter = 76, f_k = 7030.845791455904\n",
      "iter = 77, f_k = 7026.746906537194\n",
      "iter = 78, f_k = 7022.738364431493\n",
      "iter = 79, f_k = 7018.817217002883\n",
      "iter = 80, f_k = 7014.980644139553\n",
      "iter = 81, f_k = 7011.225946120782\n",
      "iter = 82, f_k = 7007.550536617925\n",
      "iter = 83, f_k = 7003.951936265555\n",
      "iter = 84, f_k = 7000.4277667478445\n",
      "iter = 85, f_k = 6996.97574534737\n",
      "iter = 86, f_k = 6993.593679912419\n",
      "iter = 87, f_k = 6990.329029798803\n",
      "iter = 88, f_k = 6987.126651327046\n",
      "iter = 89, f_k = 6983.984760170859\n",
      "iter = 90, f_k = 6980.901639625634\n",
      "iter = 91, f_k = 6977.875637327757\n",
      "iter = 92, f_k = 6974.905162193985\n",
      "iter = 93, f_k = 6971.98868156216\n",
      "iter = 94, f_k = 6969.124718515499\n",
      "iter = 95, f_k = 6966.311849374081\n",
      "iter = 96, f_k = 6963.548701340132\n",
      "iter = 97, f_k = 6960.833950283242\n",
      "iter = 98, f_k = 6958.166318654508\n",
      "iter = 99, f_k = 6955.544573519008\n",
      "iter = 100, f_k = 6952.967524697258\n"
     ]
    }
   ],
   "source": [
    "aux_prob_method = 1\n",
    "xans_tensor2, farr_tensor2 = Tensor2_iter_hist(NumIter, AuxMaxIter, w0, logistic_loss, grad_logistic_loss, hess_logistic_loss, tau, L2, eps, \n",
    "                                                           aux_prob_method, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13862.9436112 ,  11672.89722463,  10627.36123261,   9943.21745744,\n",
       "         9457.69914087,   9106.55473715,   8848.53801395,   8654.4644764 ,\n",
       "         8503.92239712,   8383.00314383,   8282.46836374,   8196.30582443,\n",
       "         8120.64735041,   8052.9989786 ,   7991.72645773,   7935.72538996,\n",
       "         7884.21724078,   7836.62605561,   7792.50489421,   7751.49207021,\n",
       "         7713.28489153,   7677.62382664,   7644.28224529,   7613.05983938,\n",
       "         7583.77809246,   7556.27691071,   7530.41206613,   7506.05316454,\n",
       "         7483.08198384,   7461.39108689,   7440.88264612,   7421.46743684,\n",
       "         7403.06396714,   7385.59772026,   7369.00049044,   7353.20979694,\n",
       "         7338.16836403,   7323.82365693,   7310.12746563,   7297.03552981,\n",
       "         7284.50719958,   7272.50512742,   7260.99498762,   7249.94522021,\n",
       "         7239.32679679,   7229.11300599,   7219.27925683,   7209.80289834,\n",
       "         7200.66305409,   7191.84047028,   7183.31737652,   7175.07735815,\n",
       "         7167.10523925,   7159.38697561,   7151.90955672,   7144.66091636,\n",
       "         7137.6298508 ,   7130.80594434,   7124.17950133,   7117.74148432,\n",
       "         7111.48345783,   7105.39753714,   7099.47634182,   7093.71708303,\n",
       "         7088.11466177,   7082.66287121,   7077.35585496,   7072.18807904,\n",
       "         7067.15430638,   7062.24957388,   7057.46917161,   7052.80862402,\n",
       "         7048.20869228,   7043.71608544,   7039.3270732 ,   7035.03810368,\n",
       "         7030.84579146,   7026.74690654,   7022.73836443,   7018.817217  ,\n",
       "         7014.98064414,   7011.22594612,   7007.55053662,   7003.95193627,\n",
       "         7000.42776675,   6996.97574535,   6993.59367991,   6990.3290298 ,\n",
       "         6987.12665133,   6983.98476017,   6980.90163963,   6977.87563733,\n",
       "         6974.90516219,   6971.98868156,   6969.12471852,   6966.31184937,\n",
       "         6963.54870134,   6960.83395028,   6958.16631865,   6955.54457352,\n",
       "         6952.9675247 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accelerated tensor method $[1]$.$[3.12]$, $p = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, f_k = 13862.943611201723\n",
      "iter = 1, f_k = 11672.897224633785\n",
      "iter = 2, f_k = 11521.953025756304\n",
      "iter = 3, f_k = 11286.67095079167\n",
      "iter = 4, f_k = 11056.545998817928\n",
      "iter = 5, f_k = 10843.25965212269\n",
      "iter = 6, f_k = 10647.948758465018\n",
      "iter = 7, f_k = 10469.542633477862\n",
      "iter = 8, f_k = 10306.526504894824\n",
      "iter = 9, f_k = 10157.384219942294\n",
      "iter = 10, f_k = 10020.7124921384\n",
      "iter = 11, f_k = 9895.243916643518\n",
      "iter = 12, f_k = 9779.842935130799\n",
      "iter = 13, f_k = 9673.495051454303\n",
      "iter = 14, f_k = 9575.294578177662\n",
      "iter = 15, f_k = 9484.4330100999\n",
      "iter = 16, f_k = 9400.188456639768\n",
      "iter = 17, f_k = 9321.916183934149\n",
      "iter = 18, f_k = 9249.040189686279\n",
      "iter = 19, f_k = 9181.04570503351\n",
      "iter = 20, f_k = 9117.472519990282\n",
      "iter = 21, f_k = 9057.909039430357\n",
      "iter = 22, f_k = 9001.986987820821\n",
      "iter = 23, f_k = 8949.376691023479\n",
      "iter = 24, f_k = 8899.782872087422\n",
      "iter = 25, f_k = 8852.94090528988\n",
      "iter = 26, f_k = 8808.613478939526\n",
      "iter = 27, f_k = 8766.58762289431\n",
      "iter = 28, f_k = 8726.672061503596\n",
      "iter = 29, f_k = 8688.694856878192\n",
      "iter = 30, f_k = 8652.501279940216\n",
      "iter = 31, f_k = 8617.952034228705\n",
      "iter = 32, f_k = 8584.92149478275\n",
      "iter = 33, f_k = 8553.296282031239\n",
      "iter = 34, f_k = 8522.973936824943\n",
      "iter = 35, f_k = 8493.861751852324\n",
      "iter = 36, f_k = 8465.875732509807\n",
      "iter = 37, f_k = 8438.939672972392\n",
      "iter = 38, f_k = 8412.984334690384\n",
      "iter = 39, f_k = 8387.946715935615\n",
      "iter = 40, f_k = 8363.769402248085\n",
      "iter = 41, f_k = 8340.399988736299\n",
      "iter = 42, f_k = 8317.790566177522\n",
      "iter = 43, f_k = 8295.897263743702\n",
      "iter = 44, f_k = 8274.679841978392\n",
      "iter = 45, f_k = 8254.101330354148\n",
      "iter = 46, f_k = 8234.12770437602\n",
      "iter = 47, f_k = 8214.727597759515\n",
      "iter = 48, f_k = 8195.872045719861\n",
      "iter = 49, f_k = 8177.53425585324\n",
      "iter = 50, f_k = 8159.689403495867\n",
      "iter = 51, f_k = 8142.314448800737\n",
      "iter = 52, f_k = 8125.3879730854715\n",
      "iter = 53, f_k = 8108.8900322895415\n",
      "iter = 54, f_k = 8092.802025625104\n",
      "iter = 55, f_k = 8077.106577726486\n",
      "iter = 56, f_k = 8061.787126027095\n",
      "iter = 57, f_k = 8046.8288081404025\n",
      "iter = 58, f_k = 8032.217319371171\n",
      "iter = 59, f_k = 8017.939218893669\n",
      "iter = 60, f_k = 8003.981859486406\n",
      "iter = 61, f_k = 7990.333324171253\n",
      "iter = 62, f_k = 7976.982368970585\n",
      "iter = 63, f_k = 7963.918371109314\n",
      "iter = 64, f_k = 7951.131282073588\n",
      "iter = 65, f_k = 7938.611585009361\n",
      "iter = 66, f_k = 7926.350256002558\n",
      "iter = 67, f_k = 7914.338728838928\n",
      "iter = 68, f_k = 7902.568862884753\n",
      "iter = 69, f_k = 7891.032913771662\n",
      "iter = 70, f_k = 7879.72350660379\n",
      "iter = 71, f_k = 7868.633611436339\n",
      "iter = 72, f_k = 7857.756520803423\n",
      "iter = 73, f_k = 7847.085829095365\n",
      "iter = 74, f_k = 7836.615413608688\n",
      "iter = 75, f_k = 7826.339417109831\n",
      "iter = 76, f_k = 7816.252231771011\n",
      "iter = 77, f_k = 7806.355373497504\n",
      "iter = 78, f_k = 7796.635792948059\n",
      "iter = 79, f_k = 7787.082480442581\n",
      "iter = 80, f_k = 7777.697931213957\n",
      "iter = 81, f_k = 7768.477547950859\n",
      "iter = 82, f_k = 7759.416918328006\n",
      "iter = 83, f_k = 7750.511802709753\n",
      "iter = 84, f_k = 7741.75812356946\n",
      "iter = 85, f_k = 7733.151956090606\n",
      "iter = 86, f_k = 7724.689519693114\n",
      "iter = 87, f_k = 7716.367170321185\n",
      "iter = 88, f_k = 7708.1813933745725\n",
      "iter = 89, f_k = 7700.12879719493\n",
      "iter = 90, f_k = 7692.206107034399\n",
      "iter = 91, f_k = 7684.410159451731\n",
      "iter = 92, f_k = 7676.7378970877635\n",
      "iter = 93, f_k = 7669.186363784004\n",
      "iter = 94, f_k = 7661.75270000935\n",
      "iter = 95, f_k = 7654.4341385705475\n",
      "iter = 96, f_k = 7647.228000579255\n",
      "iter = 97, f_k = 7640.131691656674\n",
      "iter = 98, f_k = 7633.142698356733\n",
      "iter = 99, f_k = 7626.258584791275\n",
      "iter = 100, f_k = 7619.476989441785\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "\n",
    "aux_prob_method = 1\n",
    "t2_acc_xans, _acc_farr =  TensorAcc2_iter_hist(NumIter, AuxMaxIter, w0, logistic_loss, grad_logistic_loss, hess_logistic_loss, tau, L2, eps, \n",
    "                                                           aux_prob_method, version, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13862.9436112 ,  11672.89722463,  11521.95302576,  11286.67095079,\n",
       "        11056.54599882,  10843.25965212,  10647.94875847,  10469.54263348,\n",
       "        10306.52650489,  10157.38421994,  10020.71249214,   9895.24391664,\n",
       "         9779.84293513,   9673.49505145,   9575.29457818,   9484.4330101 ,\n",
       "         9400.18845664,   9321.91618393,   9249.04018969,   9181.04570503,\n",
       "         9117.47251999,   9057.90903943,   9001.98698782,   8949.37669102,\n",
       "         8899.78287209,   8852.94090529,   8808.61347894,   8766.58762289,\n",
       "         8726.6720615 ,   8688.69485688,   8652.50127994,   8617.95203423,\n",
       "         8584.92149478,   8553.29628203,   8522.97393682,   8493.86175185,\n",
       "         8465.87573251,   8438.93967297,   8412.98433469,   8387.94671594,\n",
       "         8363.76940225,   8340.39998874,   8317.79056618,   8295.89726374,\n",
       "         8274.67984198,   8254.10133035,   8234.12770438,   8214.72759776,\n",
       "         8195.87204572,   8177.53425585,   8159.6894035 ,   8142.3144488 ,\n",
       "         8125.38797309,   8108.89003229,   8092.80202563,   8077.10657773,\n",
       "         8061.78712603,   8046.82880814,   8032.21731937,   8017.93921889,\n",
       "         8003.98185949,   7990.33332417,   7976.98236897,   7963.91837111,\n",
       "         7951.13128207,   7938.61158501,   7926.350256  ,   7914.33872884,\n",
       "         7902.56886288,   7891.03291377,   7879.7235066 ,   7868.63361144,\n",
       "         7857.7565208 ,   7847.0858291 ,   7836.61541361,   7826.33941711,\n",
       "         7816.25223177,   7806.3553735 ,   7796.63579295,   7787.08248044,\n",
       "         7777.69793121,   7768.47754795,   7759.41691833,   7750.51180271,\n",
       "         7741.75812357,   7733.15195609,   7724.68951969,   7716.36717032,\n",
       "         7708.18139337,   7700.12879719,   7692.20610703,   7684.41015945,\n",
       "         7676.73789709,   7669.18636378,   7661.75270001,   7654.43413857,\n",
       "         7647.22800058,   7640.13169166,   7633.14269836,   7626.25858479,\n",
       "         7619.47698944])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_acc_farr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accelerated tensor method $[1]$.$[3.12]$, $p = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, f_k = 13862.943611201723\n",
      "iter = 1, f_k = 11421.663356650139\n",
      "iter = 2, f_k = 11332.495843025788\n",
      "iter = 3, f_k = 11132.037529963967\n",
      "iter = 4, f_k = 10918.034976914088\n",
      "iter = 5, f_k = 10711.910485697274\n",
      "iter = 6, f_k = 10519.04836038766\n",
      "iter = 7, f_k = 10340.431222822077\n",
      "iter = 8, f_k = 10175.648447859036\n",
      "iter = 9, f_k = 10023.829502952834\n",
      "iter = 10, f_k = 9883.971927393804\n",
      "iter = 11, f_k = 9755.066444268705\n",
      "iter = 12, f_k = 9636.146677173572\n",
      "iter = 13, f_k = 9526.308514719029\n",
      "iter = 14, f_k = 9424.716510657972\n",
      "iter = 15, f_k = 9330.604243042268\n",
      "iter = 16, f_k = 9243.27215591225\n",
      "iter = 17, f_k = 9162.08395712279\n",
      "iter = 18, f_k = 9086.462625110382\n",
      "iter = 19, f_k = 9015.88606622171\n",
      "iter = 20, f_k = 8949.88292011627\n",
      "iter = 21, f_k = 8888.028371751556\n",
      "iter = 22, f_k = 8829.940144322141\n",
      "iter = 23, f_k = 8775.274747117315\n",
      "iter = 24, f_k = 8723.723872721192\n",
      "iter = 25, f_k = 8675.011031547157\n",
      "iter = 26, f_k = 8628.888455147766\n",
      "iter = 27, f_k = 8585.134194737417\n",
      "iter = 28, f_k = 8543.549488563587\n",
      "iter = 29, f_k = 8503.956305391905\n",
      "iter = 30, f_k = 8466.195119112104\n",
      "iter = 31, f_k = 8430.122929244622\n",
      "iter = 32, f_k = 8395.611418357936\n",
      "iter = 33, f_k = 8362.545322781476\n",
      "iter = 34, f_k = 8330.820979750162\n",
      "iter = 35, f_k = 8300.345004063858\n",
      "iter = 36, f_k = 8271.033136415028\n",
      "iter = 37, f_k = 8242.809203616693\n",
      "iter = 38, f_k = 8215.60420915504\n",
      "iter = 39, f_k = 8189.3555322767925\n",
      "iter = 40, f_k = 8164.006186927318\n",
      "iter = 41, f_k = 8139.504229289138\n",
      "iter = 42, f_k = 8115.802184329008\n",
      "iter = 43, f_k = 8092.8565623542945\n",
      "iter = 44, f_k = 8070.6274324711885\n",
      "iter = 45, f_k = 8049.078044267622\n",
      "iter = 46, f_k = 8028.174499067433\n",
      "iter = 47, f_k = 8007.885455580277\n",
      "iter = 48, f_k = 7988.181870102276\n",
      "iter = 49, f_k = 7969.036778177686\n",
      "iter = 50, f_k = 7950.425086010954\n",
      "iter = 51, f_k = 7932.323393697375\n",
      "iter = 52, f_k = 7914.709840494934\n",
      "iter = 53, f_k = 7897.563970164047\n",
      "iter = 54, f_k = 7880.8665926472495\n",
      "iter = 55, f_k = 7864.599684056821\n",
      "iter = 56, f_k = 7848.746287558467\n",
      "iter = 57, f_k = 7833.290424073553\n",
      "iter = 58, f_k = 7818.217013923146\n",
      "iter = 59, f_k = 7803.511804312008\n",
      "iter = 60, f_k = 7789.161306138419\n",
      "iter = 61, f_k = 7775.152735556015\n",
      "iter = 62, f_k = 7761.473964616695\n",
      "iter = 63, f_k = 7748.113472512133\n",
      "iter = 64, f_k = 7735.060302061612\n",
      "iter = 65, f_k = 7722.30402862575\n",
      "iter = 66, f_k = 7709.834711887115\n",
      "iter = 67, f_k = 7697.642873219607\n",
      "iter = 68, f_k = 7685.719463105426\n",
      "iter = 69, f_k = 7674.055839255779\n",
      "iter = 70, f_k = 7662.643731411604\n",
      "iter = 71, f_k = 7651.475229554522\n",
      "iter = 72, f_k = 7640.542755450391\n",
      "iter = 73, f_k = 7629.83904823824\n",
      "iter = 74, f_k = 7619.357145693943\n",
      "iter = 75, f_k = 7609.09036698364\n",
      "iter = 76, f_k = 7599.0322999353575\n",
      "iter = 77, f_k = 7589.1767782725865\n",
      "iter = 78, f_k = 7579.517882692834\n",
      "iter = 79, f_k = 7570.049917880303\n",
      "iter = 80, f_k = 7560.767401934966\n",
      "iter = 81, f_k = 7551.665062665527\n",
      "iter = 82, f_k = 7542.737817454882\n",
      "iter = 83, f_k = 7533.98077438063\n",
      "iter = 84, f_k = 7525.389218038365\n",
      "iter = 85, f_k = 7516.958601994186\n",
      "iter = 86, f_k = 7508.684539047708\n",
      "iter = 87, f_k = 7500.562797129443\n",
      "iter = 88, f_k = 7492.589293417964\n",
      "iter = 89, f_k = 7484.760084927238\n",
      "iter = 90, f_k = 7477.071362139741\n",
      "iter = 91, f_k = 7469.5194460280045\n",
      "iter = 92, f_k = 7462.100781097978\n",
      "iter = 93, f_k = 7454.8119301378165\n",
      "iter = 94, f_k = 7447.6495697511\n",
      "iter = 95, f_k = 7440.610485855117\n",
      "iter = 96, f_k = 7433.691569067155\n",
      "iter = 97, f_k = 7426.889809631572\n",
      "iter = 98, f_k = 7420.202295959028\n",
      "iter = 99, f_k = 7413.626207140165\n",
      "iter = 100, f_k = 7407.1588146278855\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "aux_prob_method = 1\n",
    "xans_tensor3_acc, farr_tensor3_acc = TensorAcc3_iter_hist(NumIter, TensorMaxIter, AuxMaxIter, w0, logistic_loss, grad_logistic_loss,\n",
    "            hess_logistic_loss, dot_tensor3_and_vector_vector_logistic_loss, tau, L3, eps, aux_prob_method, version, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13862.9436112 ,  11421.66335665,  11332.49584303,  11132.03752996,\n",
       "        10918.03497691,  10711.9104857 ,  10519.04836039,  10340.43122282,\n",
       "        10175.64844786,  10023.82950295,   9883.97192739,   9755.06644427,\n",
       "         9636.14667717,   9526.30851472,   9424.71651066,   9330.60424304,\n",
       "         9243.27215591,   9162.08395712,   9086.46262511,   9015.88606622,\n",
       "         8949.88292012,   8888.02837175,   8829.94014432,   8775.27474712,\n",
       "         8723.72387272,   8675.01103155,   8628.88845515,   8585.13419474,\n",
       "         8543.54948856,   8503.95630539,   8466.19511911,   8430.12292924,\n",
       "         8395.61141836,   8362.54532278,   8330.82097975,   8300.34500406,\n",
       "         8271.03313642,   8242.80920362,   8215.60420916,   8189.35553228,\n",
       "         8164.00618693,   8139.50422929,   8115.80218433,   8092.85656235,\n",
       "         8070.62743247,   8049.07804427,   8028.17449907,   8007.88545558,\n",
       "         7988.1818701 ,   7969.03677818,   7950.42508601,   7932.3233937 ,\n",
       "         7914.70984049,   7897.56397016,   7880.86659265,   7864.59968406,\n",
       "         7848.74628756,   7833.29042407,   7818.21701392,   7803.51180431,\n",
       "         7789.16130614,   7775.15273556,   7761.47396462,   7748.11347251,\n",
       "         7735.06030206,   7722.30402863,   7709.83471189,   7697.64287322,\n",
       "         7685.71946311,   7674.05583926,   7662.64373141,   7651.47522955,\n",
       "         7640.54275545,   7629.83904824,   7619.35714569,   7609.09036698,\n",
       "         7599.03229994,   7589.17677827,   7579.51788269,   7570.04991788,\n",
       "         7560.76740193,   7551.66506267,   7542.73781745,   7533.98077438,\n",
       "         7525.38921804,   7516.95860199,   7508.68453905,   7500.56279713,\n",
       "         7492.58929342,   7484.76008493,   7477.07136214,   7469.51944603,\n",
       "         7462.1007811 ,   7454.81193014,   7447.64956975,   7440.61048586,\n",
       "         7433.69156907,   7426.88980963,   7420.20229596,   7413.62620714,\n",
       "         7407.15881463])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr_tensor3_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading data computing in LogisticRegression-Part2.ipynb with $p = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13862.9436112 ,  11421.66335665,  10254.52009982,   9516.40449922,\n",
       "         9024.19160974,   8691.00855211,   8456.96422112,   8282.48930278,\n",
       "         8143.95084358,   8028.36642997,   7928.83386348,   7841.55450955,\n",
       "         7764.23246038,   7695.30588465,   7633.59822708,   7578.16129401,\n",
       "         7528.20135516,   7483.04053942,   7442.09371398,   7404.85281694,\n",
       "         7370.87530869,   7339.77518681,   7311.21572505,   7284.90341643,\n",
       "         7260.58276691,   7238.03172239,   7217.05757734,   7197.49328203,\n",
       "         7179.19410004,   7162.03459547,   7145.90592992,   7130.71346653,\n",
       "         7116.37466613,   7102.81725879,   7089.97767331,   7077.79970141,\n",
       "         7066.23337057,   7055.23400208,   7044.76142714,   7034.77933965,\n",
       "         7025.25476235,   7016.15760754,   7007.46031645,   6999.13756178,\n",
       "         6991.1660017 ,   6983.52407512,   6976.1918299 ,   6969.15077688,\n",
       "         6962.38376593,   6955.87487652,   6949.60932231,   6943.57336622,\n",
       "         6937.75424344,   6932.14009135,   6926.71988491,   6921.48337693,\n",
       "         6916.4210423 ,   6911.52402594,   6906.78409396,   6902.19358785,\n",
       "         6897.74538161,   6893.43284153,   6889.24978836,   6885.19046217,\n",
       "         6881.24949007,   6877.42185479,   6873.702867  ,   6870.08813887,\n",
       "         6866.5735599 ,   6863.15527451,   6859.82966152,   6856.59331528,\n",
       "         6853.44302831,   6850.37577553,   6847.38869969,   6844.47909827,\n",
       "         6841.64441133,   6838.88221059,   6836.19018946,   6833.5661538 ,\n",
       "         6831.00801404,   6828.51377732,   6826.0815407 ,   6823.70948501,\n",
       "         6821.39586922,   6819.13902528,   6816.93735341,   6814.78931782,\n",
       "         6812.69344284,   6810.64830931,   6808.65255135,   6806.7048533 ,\n",
       "         6804.80394705,   6802.94860942,   6801.13765987,   6799.36995832,\n",
       "         6797.64440315,   6795.95992937,   6794.31550684,   6792.71013869,\n",
       "         6791.14285974])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr_tensor_3 = np.loadtxt(\"tensor3_result_log_regr.txt\", delimiter=\",\", unpack=False)\n",
    "farr_tensor_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting graphs for $[1]$.$[2.16]$ and $[1]$.$[3.12]$ with $p = 3$ and $p = 2$ by testing on $[1]$.$[4.3]$ on logistic regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ee5146630>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.xlabel(r'$Iterations$') \n",
    "plt.ylabel(r'$logistic \\; loss$') \n",
    "plt.title(r'$features = {0}, objects = {1}$'.format(features_num, train_object_size)) \n",
    "\n",
    "arr_i = [i for i in range(NumIter + 1)]\n",
    "\n",
    "plt.plot(arr_i, farr_tensor_3, label = 'p = 3')\n",
    "plt.plot(arr_i, farr_tensor3_acc, label = 'p = 3, accel')\n",
    "plt.plot(arr_i, farr_tensor2, label = 'p = 2')\n",
    "plt.plot(arr_i, _acc_farr, label = 'p = 2, accel')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True) \n",
    "fig.savefig('tensors_logistic_loss_iter.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('farr_tensor_3_log_regr.txt', farr_tensor_3)\n",
    "np.savetxt('farr_tensor3_acc_log_regr.txt', farr_tensor3_acc)\n",
    "np.savetxt('farr_tensor2_log_regr.txt', farr_tensor2)\n",
    "np.savetxt('farr_tensor2_acc_log_regr.txt', _acc_farr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
