{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[1]$ --- Implementable tensor methods in unconstrained convex optimization, https://alfresco.uclouvain.be/alfresco/service/guest/streamDownload/workspace/SpacesStore/aabc2323-0bc1-40d4-9653-1c29971e7bd8/coredp2018_05web.pdf?guest=true.\n",
    "\n",
    "$[2]$ ---  Cubic regularization of Newton method and its global performance, http://lab7.ipu.ru/files/polyak/Nest_Pol-MathProg'06.pdf.\n",
    "\n",
    "$[3]$ --- Acelerating the cubic regularization of Newton’s method on convex\n",
    "problems, http://webdoc.sub.gwdg.de/ebook/serien/e/CORE/dp2005_68.pdf.\n",
    "\n",
    "$[4]$ --- A.R. Conn, N.I. M. Gould, and Ph.L. Toint. Trust Region Methods. SIAM, Philadelphia, 2000.\n",
    "\n",
    "$[5]$ --- Анализ быстрого градиентного метода Нестерова для задач машинного обучения с $L_1$-регуляризацией, http://www.machinelearning.ru/wiki/images/0/03/Rodomanov_FGM.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of code: compute method $[1]$.$[2.16]$ with $p = 3$ by testing on logistic regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd.numpy import linalg\n",
    "from autograd import grad    \n",
    "from autograd import jacobian\n",
    "from numpy import linalg\n",
    "import math\n",
    "import autograd.scipy as scipy\n",
    "from scipy import optimize\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation parameters $c, A \\succeq 0, \\gamma > 0$ for auxiliary minimization problem $[1]$.$[5.8]$ in terms of problem \n",
    "$$\n",
    "\\langle c, h \\rangle + \\frac{1}{2}\\langle Ah, h \\rangle + \\frac{\\gamma}{4}||h||_2^4 \\rightarrow \\min\\limits_{h \\in \\mathbb{R}^n}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cur_params(grad_f_cur, hessian_f_cur, tensor3_f_cur_vect, hk, tau, L):\n",
    "    grad_d = np.dot(hk, hk.T)*hk\n",
    "    c = (grad_f_cur - 1/tau*np.dot(hessian_f_cur, hk.T) + 0.5*tensor3_f_cur_vect \n",
    "                                                                        - tau*(tau + 1)/2*L*grad_d) + tau*tau*L/2*grad_d\n",
    "    \n",
    "    A = (tau + 1) / tau * hessian_f_cur\n",
    "    \n",
    "    gamma = tau*(tau + 1)/2*L\n",
    "    return c, A, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function $f_{c, A, \\gamma}(h) = \\langle c, h \\rangle + \\frac{1}{2}\\langle Ah, h \\rangle + \\frac{\\gamma}{4}||h||_2^4$ for auxiliary minimization problem $[1]$.$[5.8]$ and its univariate dual function $g_{c, A, \\gamma}(\\tau) = \\frac{1}{2}\\tau^2 + \\frac{1}{2}\\Bigl\\langle \\bigl(\\sqrt{2\\gamma}\\tau B + A\\bigr)^{-1}c, c \\Bigr\\rangle$ with $B = I_{n \\times n}$ for Euclid 2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_problem_func(h, *args):\n",
    "    c, A, gamma = args\n",
    "    d = linalg.norm(h)\n",
    "    d4 = d*d*d*d\n",
    "    return np.dot(c, h) + 0.5 * np.dot(np.dot(A, h), h) + 0.25*gamma*d4 \n",
    "\n",
    "def aux_problem_onedim_func(tau, *args):\n",
    "    c, A, B, gamma = args\n",
    "    sgam = math.sqrt(2*gamma)\n",
    "    S = sgam*tau*B + A\n",
    "    invS = linalg.inv(S)\n",
    "    f = 0.5*tau*tau + 0.5 * np.dot(np.dot(invS, c), c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation minimum of $f_{c, A, \\gamma}(h)$ using minimum of dual function $g_{c, A, \\gamma}(\\tau)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hopt(t, *args):\n",
    "    c, A, B, gamma = args\n",
    "    S = math.sqrt(2*gamma)*t*B + A\n",
    "    invS = linalg.inv(S)\n",
    "    h_opt = -np.dot(invS, c)\n",
    "    return h_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newton's method for finding minimum $g_{c, A, \\gamma}(\\tau)$ based on $[4]$, Chapter $7$.\n",
    "\n",
    "The main idea is to find root $\\lambda > 0$ of first-order optimality condition: $\\sqrt{\\lambda} = ||s(\\lambda)||_2, s(\\lambda) = -(A + \\lambda I)^{-1}c_{\\text{new}}, \\lambda = \\sqrt{2\\gamma}\\tau, c_{\\text{new}} = \\sqrt{\\gamma}c$.\n",
    "\n",
    "It is better to solve the secular equation:\n",
    "$$\n",
    "\\phi(\\lambda) = \\frac{1}{||s(\\lambda)||_{2}} - \\frac{1}{\\sqrt{\\lambda}}.\n",
    "$$\n",
    "So \n",
    "$$\n",
    "\\phi^{'}(\\lambda) = \\frac{\\bigl \\langle s(\\lambda), A(\\lambda)^{-1}s(\\lambda) \\bigr \\rangle}{||s(\\lambda)||_2^3} + \\frac{1}{2\\lambda^{\\frac{3}{2}}} \\geq 0, A(\\lambda) = A + \\lambda I;\n",
    "$$\n",
    "$$\n",
    "\\phi^{''}(\\lambda) = \\frac{3\\Bigl(\\bigl \\langle s(\\lambda), \\nabla_{\\lambda}s(\\lambda) \\bigr \\rangle^2 - ||s(\\lambda)||_2^2||\\nabla_{\\lambda}s(\\lambda)||_2^2\\Bigr)}{||s(\\lambda)||_2^5} - \\frac{3}{4\\lambda^{\\frac{5}{2}}} < 0\n",
    "$$ by Cauchy-Schwartz inequality.\n",
    "\n",
    "Newton's method for finding a root of the scalar equation $\\phi(\\lambda) = 0$ replaces  the estimate $\\lambda_k > 0$ with the improved estimate $\\lambda_{k + 1}$ for which\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k - \\frac{\\phi(\\lambda_k)}{\\phi^{'}(\\lambda_k)}.\n",
    "$$\n",
    "Using Cholesky factors $A(\\lambda) = L(\\lambda)L^T(\\lambda)$ we can write:\n",
    "$$\n",
    "\\bigl \\langle s, A(\\lambda)^{-1}s \\bigr \\rangle = \\bigl \\langle s, L^{-T}L^{-1}s \\bigr \\rangle = \\bigl \\langle L^{-1}s, L^{-1}s \\bigr \\rangle = ||w(\\lambda)||_2^2\n",
    "$$\n",
    "with $w(\\lambda) = L^{-1}(\\lambda)s(\\lambda)$.\n",
    "The complete Newton algorithm is:\n",
    "$$\n",
    "\\lambda_{k + 1} = \\lambda_k \\cdot \\Bigl(1 - 2||s(\\lambda_k)||_2^2\\frac{\\sqrt{\\lambda_k} - ||s(\\lambda_k)||_2}{||s(\\lambda_k)||_2^3 + 2||w(\\lambda_k)||_2^2\\lambda_k^{\\frac{3}{2}}}\\Bigr) \n",
    "$$ while $\\lambda_{k + 1} > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_step_tensor3(AuxMaxIter, *args):\n",
    "    c, H, gamma, eps = args\n",
    "    c_new = c*(gamma**0.5)\n",
    "    lambda_ans = eps\n",
    "    k = 0\n",
    "    while (k < AuxMaxIter):\n",
    "        H_lambda = H + lambda_ans*np.identity(H.shape[0])\n",
    "        try:\n",
    "            L = linalg.cholesky(H_lambda)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        s = linalg.solve(H_lambda, -c_new)\n",
    "        w = linalg.solve(L, s)\n",
    "        lambda_next = lambda_ans*(1 - \n",
    "            2*(lambda_ans**0.5 - linalg.norm(s))*(linalg.norm(s)**2)/\n",
    "                                  (linalg.norm(s)**3 + 2*(linalg.norm(w)**2)*(lambda_ans**(1.5))))\n",
    "        if (lambda_next > 0):\n",
    "            lambda_ans = lambda_next\n",
    "        else:\n",
    "            break\n",
    "        k += 1\n",
    "    #tau_opt2 = 0\n",
    "    h_opt1 = -np.dot(linalg.inv(H + lambda_ans*np.identity(H.shape[0])), c)\n",
    "    #h_opt2 = -np.dot(linalg.pinv(H), c)\n",
    "    #val_1 = np.dot(c, h_opt1) + 1/2*np.dot(np.dot(H, h_opt1), h_opt1) + gamma/4*(linalg.norm(h_opt1)**4)\n",
    "    #val_2 = np.dot(c, h_opt2) + 1/2*np.dot(np.dot(H, h_opt2), h_opt2) + gamma/4*(linalg.norm(h_opt2)**4)\n",
    "    #if (val_1 < val_2):\n",
    "     #   h_opt = h_opt1\n",
    "    #else:\n",
    "      #  h_opt = h_opt2\n",
    "    #return h_opt\n",
    "    return h_opt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration of process $[1].[5.8]$ to solve auxiliary minimization problem $[1]$.$[5.4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T3M(x, aux_prob_method, grad_f_cur, hess_f_cur, tensor3_f_direct, TensorMaxIter, AuxMaxIter, tau, L, eps, *args):\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    hk = np.zeros(n)\n",
    "    B = np.eye(n)\n",
    " \n",
    "    if aux_prob_method == 1:\n",
    "        xb = L*1e10\n",
    "        t0 = 0+eps\n",
    "    \n",
    "    for aux_k in range(1, TensorMaxIter + 1):\n",
    "        tensor3_f_cur_vect = tensor3_f_direct(x, hk, *args)\n",
    "        c, A, gamma = calc_cur_params(grad_f_cur, hess_f_cur, tensor3_f_cur_vect, hk, tau, L)\n",
    "            \n",
    "        if aux_prob_method == 1:\n",
    "            allargs = (c, A, B, gamma)\n",
    "\n",
    "            res = optimize.minimize_scalar(aux_problem_onedim_func, args = allargs, method = 'bounded', bounds=(t0, xb), \n",
    "                                                options={'xatol': eps, 'maxiter': AuxMaxIter, 'disp': False})\n",
    "            \n",
    "            tau_opt = res.x\n",
    "            \n",
    "            h_opt = calc_hopt(tau_opt, *(c, A, B, gamma))\n",
    "                \n",
    "        elif aux_prob_method == 2:\n",
    "            allargs = (c, A, gamma)\n",
    "        \n",
    "            res = optimize.minimize(aux_problem_func, args = allargs, tol = eps, x0 = hk, method = 'Powell', \n",
    "                                                options={'ftol': eps,'maxiter': AuxMaxIter, 'disp': False})                \n",
    "            h_opt = res.x\n",
    "                \n",
    "        elif aux_prob_method == 3:\n",
    "            h_opt = Newton_step_tensor3(AuxMaxIter, *(c, A, gamma, eps))\n",
    "        hk = h_opt\n",
    "    return (hk + x)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor method $[1]$.$[2.16]$ by Yu. Nesterov (2018), $p = 3$\n",
    "Parameters:  \n",
    "$NumIter$ --- max number of steps in tensor method.\n",
    "\n",
    "$TensorMaxIter$ --- max number of iterations in auxiliary problem solution method $[1]$.$[5.8]$.\n",
    "\n",
    "$AuxMaxIter$ --- max number of iterations to perform step $[1]$.$[5.8]$.\n",
    "\n",
    "$x_0$ --- initial point.\n",
    "\n",
    "$f$ --- objective function oracle.  \n",
    "\n",
    "$\\nabla f$ --- objective gradient of $f$ oracle.\n",
    "\n",
    "$\\nabla^2 f$ --- objective hessian of $f$ oracle.\n",
    "\n",
    "$\\langle \\nabla^3 f(x) h, h \\rangle$ --- objective third directional derivative of $f$ at $x$ along direction $h$ oracle .\n",
    "\n",
    "$\\tau > 1$ - parameter in $[1]$.$[5.5]$.\n",
    "\n",
    "$L_3(f)$ - uniform bound for the Lipschitz constant of third derivative.\n",
    "\n",
    "$\\varepsilon$ - parameter to perform step $[1]$.$[5.8]$, meaning depends of $aux\\_prob\\_method$.\n",
    "\n",
    "$aux\\_prob\\_method$ --- how to solve auxiliary problem (5.4): $1$ --- by means of minimization $g_{c, A, \\gamma}(\\tau)$ using $scipy.optimize$, $2$ --- by means of minimization $f_{c, A, \\gamma}(h)$ using $scipy.optimize$, $3$ --- by means of minimization $g_{c, A, \\gamma}(\\tau)$ using Newton's method.\n",
    "\n",
    "$args$ - arguments for parametrization of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor3_iter(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                                             aux_prob_method, *args):\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    xk = copy.deepcopy(x0)\n",
    "\n",
    "    for k in range(NumIter):\n",
    "        f_xk = f(xk, *args)\n",
    "        grad_f_xk = grad_f(xk, *args)\n",
    "        hessian_f_xk = hess_f(xk, *args)\n",
    "        farr[k] = f_xk\n",
    "        #fgradarr[k] = linalg.norm(grad_f_xk)\n",
    "        xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter,\n",
    "                                                             tau, L, eps, *args) \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    #grad_f_xk = grad_f(xk, *args)\n",
    "    farr[NumIter] = f_xk\n",
    "    #fgradarr[NumIter] = linalg.norm(grad_f_xk)\n",
    "    \n",
    "    return xk, f_xk, farr#, fgradarr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor method $[1]$.$[2.16]$ by Yu. Nesterov (2018), $p = 3$ with history printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor3_iter_hist(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                                             aux_prob_method, *args):\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    xk = copy.deepcopy(x0)\n",
    "\n",
    "    for k in range(NumIter):\n",
    "        f_xk = f(xk, *args)\n",
    "        print(\"iter = {0}, f_k = {1}\".format(k, f_xk))\n",
    "        grad_f_xk = grad_f(xk, *args)\n",
    "        hessian_f_xk = hess_f(xk, *args)\n",
    "        farr[k] = f_xk\n",
    "        #fgradarr[k] = linalg.norm(grad_f_xk)\n",
    "        xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter,\n",
    "                                                             tau, L, eps, *args) \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    print(\"iter = {0}, f_k = {1}\".format(k, f_xk))\n",
    "    #grad_f_xk = grad_f(xk, *args)\n",
    "    farr[NumIter] = f_xk\n",
    "    #fgradarr[NumIter] = linalg.norm(grad_f_xk)\n",
    "    \n",
    "    return xk, f_xk, farr#, fgradarr\n",
    "\n",
    "def TensorAcc3_iter_hist(NumIter, TensorMaxIter, AuxMaxIter, x0, f, grad_f, hess_f, tensor3_f_direct, tau, L, eps, \n",
    "                                               aux_prob_method, version, *args):\n",
    "    \n",
    "    xk = copy.deepcopy(x0)\n",
    "    M = tau*tau*L\n",
    "    \n",
    "    farr = np.zeros(NumIter + 1)\n",
    "    #fgradarr = np.zeros(NumIter + 1)\n",
    "    \n",
    "    \n",
    "    f_xk = f(xk, *args)\n",
    "    print(\"iter = {0}, f_k = {1}\".format(0, f_xk))\n",
    "    farr[0] = f_xk\n",
    "    grad_f_xk = grad_f(xk, *args)\n",
    "    #fgradarr[0] = linalg.norm(grad_f_xk)\n",
    "    hessian_f_xk = hess_f(xk, *args)\n",
    "  \n",
    "    xk = T3M(xk, aux_prob_method, grad_f_xk, hessian_f_xk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, tau, L, \n",
    "                                                                                                         eps, *args)  \n",
    "    #print('xk = {0}'.format(xk))\n",
    "    f_xk = f(xk, *args)\n",
    "    print(\"iter = {0}, f_k = {1}\".format(1, f_xk))\n",
    "    #gradf_xk = grad_f(xk, *args)\n",
    "    farr[1] = f_xk\n",
    "    #fgradarr[1] = linalg.norm(gradf_xk)\n",
    "    \n",
    "    p = 3\n",
    "    C = p / 2 * math.sqrt((p+1)/(p-1)*(M*M - L*L))\n",
    "    \n",
    "    min_psik = copy.deepcopy(x0)\n",
    "    size = x0.shape[0]\n",
    "    sk = np.zeros(size)\n",
    "    if (version == 1):\n",
    "        ak_part = math.sqrt(pow((p - 1) * (M*M - L*L) / 4 / (p + 1) / M / M, p))\n",
    "    elif (version == 2):\n",
    "        ak_part = math.sqrt(pow((p + 1) * (M*M - L*L) / 4 / (p - 1) / M / M, p))\n",
    "    k = 1\n",
    "    Ak1 = ak_part*pow(k / (p + 1), p + 1)\n",
    "    \n",
    "    factor_p = math.factorial(p)\n",
    "    \n",
    "    for k in range(1, NumIter):\n",
    "        vk = min_psik\n",
    "        Ak = Ak1\n",
    "        Ak1 = ak_part*pow((k + 1) / (p + 1), p + 1)\n",
    "        alpha = Ak / Ak1\n",
    "        #print('alpha = {0}'.format(alpha))\n",
    "        yk = alpha * xk + (1 - alpha) * vk\n",
    "        #print('yk = {0}'.format(yk))\n",
    "        grad_f_yk = grad_f(yk, *args)\n",
    "        hessian_f_yk = hess_f(yk, *args)\n",
    "\n",
    "        xk = T3M(yk, aux_prob_method, grad_f_yk, hessian_f_yk, tensor3_f_direct, TensorMaxIter, AuxMaxIter, \n",
    "                                                                                     tau, L, eps, *args)        \n",
    "        #print('xk = {0}'.format(xk))\n",
    "        grad_next = grad_f(xk, *args)\n",
    "        a = Ak1 - Ak\n",
    "        sk = sk + a * grad_next \n",
    "        min_psik = x0 - pow(factor_p / C / pow(linalg.norm(sk), p - 1), 1. / p) * sk\n",
    "        \n",
    "        f_xk = f(xk, *args)\n",
    "        print(\"iter = {0}, f_k = {1}\".format(k + 1, f_xk))\n",
    "        farr[k + 1] = f_xk\n",
    "        #fgradarr[k + 1] = linalg.norm(grad_next)\n",
    "    \n",
    "    return xk, f_xk, farr#, fgradarr, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was read successfully!\n"
     ]
    }
   ],
   "source": [
    "features = 54\n",
    "train_object_size = 20000\n",
    "test_object_size = 30000\n",
    "read_object_size = train_object_size + test_object_size\n",
    "\n",
    "train_object = np.zeros((train_object_size, features))\n",
    "train_ans = np.zeros(train_object_size)\n",
    "\n",
    "test_object = np.zeros((test_object_size, features)) \n",
    "test_ans = np.zeros(test_object_size)\n",
    "\n",
    "f = open('covtype.libsvm.binary.scale')\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    if (line_num == read_object_size):\n",
    "        break\n",
    "    line_object = line.split()\n",
    "    len_line_object = len(line_object)\n",
    "    for i in range(1, len_line_object):\n",
    "        current_cell = line_object[i].split(':')\n",
    "        current_num_feature = int(current_cell[0]) - 1\n",
    "        current_feature = float(current_cell[1])\n",
    "        bin_class = int(line_object[0])\n",
    "        if (bin_class == 2):\n",
    "            bin_class = -1\n",
    "        if (line_num < train_object_size):\n",
    "            train_ans[line_num] = bin_class\n",
    "            train_object[line_num][current_num_feature] = current_feature\n",
    "        else:\n",
    "            test_ans[line_num - train_object_size] = bin_class\n",
    "            test_object[line_num - train_object_size][current_num_feature] = current_feature\n",
    "    line_num += 1\n",
    "f.close()\n",
    "print(\"Data was read successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logististic loss and it's gradient, hessian and third derivative using closed-form expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    return sum([np.log(1 + np.exp(-y[i]*np.dot(X[i], w))) for i in range(objects_size)])\n",
    "\n",
    "def grad_logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    return sum([-y[i]*X[i]/(1 + np.exp(y[i]*np.dot(X[i], w))) for i in range(objects_size)])\n",
    "\n",
    "def hess_logistic_loss(w, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    features = X.shape[1]\n",
    "    ans = np.zeros((features, features))\n",
    "    for i in range(objects_size):\n",
    "        ans += np.exp(y[i]*np.dot(X[i], w))/((1 + np.exp(y[i]*np.dot(X[i], w)))**2)*np.dot(X[i].reshape(-1, 1),\n",
    "                                                                X[i].reshape(1, -1))\n",
    "    return ans\n",
    "\n",
    "def dot_tensor3_and_vector_vector_logistic_loss(w, h, *args):\n",
    "    X, y = args\n",
    "    objects_size = y.shape[0]\n",
    "    features = X.shape[1]\n",
    "    ans = np.zeros(features)\n",
    "    for i in range(objects_size):\n",
    "        add = y[i]*(1 - np.exp(y[i]*np.dot(X[i], w)))*(np.dot(h, X[i])**2)*np.exp(y[i]*np.dot(X[i], w))\n",
    "        add = add/((1 + np.exp(y[i]*np.dot(X[i], w)))**3)*X[i]\n",
    "        ans += add\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upper bounds on $L_2(\\text{logistic loss})$ and $L_3(\\text{logistic loss})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_upper_bound(X):\n",
    "    return 1/10*sum([linalg.norm(X[i])**3 for i in range(X.shape[0])])\n",
    "\n",
    "def L3_upper_bound(X):\n",
    "    return 1/8*sum([linalg.norm(X[i])**4 for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our problem to solve and it's parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_const_feature = np.array([np.array([1]) for i in range(train_object_size)]) #добавим фиктивный признак\n",
    "train_extended_object = copy.deepcopy(train_object)\n",
    "train_extended_object = np.column_stack((train_extended_object, train_const_feature[:, 0]))\n",
    "args = (train_extended_object, train_ans)\n",
    "\n",
    "features_num = features + 1\n",
    "w0 = np.zeros(features_num)\n",
    "\n",
    "NumIter = 100\n",
    "TensorMaxIter = 40\n",
    "AuxMaxIter = 100\n",
    "eps = 1e-7\n",
    "tau = 1 + eps\n",
    "L2 = L2_upper_bound(train_extended_object)\n",
    "L3 = L3_upper_bound(train_extended_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28193.786578212414"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85588.5309823477"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.368684 ,  0.141667 ,  0.0454545,  0.184681 ,  0.223514 ,\n",
       "        0.0716594,  0.870079 ,  0.913386 ,  0.582677 ,  0.875366 ,\n",
       "        1.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  1.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  1.       ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extended_object[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, f_k = 13862.943611201723\n",
      "iter = 1, f_k = 11421.663356650139\n",
      "iter = 2, f_k = 10254.520099817568\n",
      "iter = 3, f_k = 9516.404499219063\n",
      "iter = 4, f_k = 9024.1916097374\n",
      "iter = 5, f_k = 8691.008552110596\n",
      "iter = 6, f_k = 8456.964221117521\n",
      "iter = 7, f_k = 8282.48930277633\n",
      "iter = 8, f_k = 8143.950843584272\n",
      "iter = 9, f_k = 8028.366429968638\n",
      "iter = 10, f_k = 7928.833863479684\n",
      "iter = 11, f_k = 7841.554509547678\n",
      "iter = 12, f_k = 7764.232460384513\n",
      "iter = 13, f_k = 7695.305884654516\n",
      "iter = 14, f_k = 7633.598227076089\n",
      "iter = 15, f_k = 7578.161294012672\n",
      "iter = 16, f_k = 7528.201355157959\n",
      "iter = 17, f_k = 7483.04053941726\n",
      "iter = 18, f_k = 7442.093713976898\n",
      "iter = 19, f_k = 7404.852816939839\n",
      "iter = 20, f_k = 7370.875308689801\n",
      "iter = 21, f_k = 7339.775186805825\n",
      "iter = 22, f_k = 7311.215725052544\n",
      "iter = 23, f_k = 7284.903416433555\n",
      "iter = 24, f_k = 7260.58276691386\n",
      "iter = 25, f_k = 7238.031722386116\n",
      "iter = 26, f_k = 7217.057577336579\n",
      "iter = 27, f_k = 7197.493282026668\n",
      "iter = 28, f_k = 7179.19410004147\n",
      "iter = 29, f_k = 7162.034595468078\n",
      "iter = 30, f_k = 7145.9059299237015\n",
      "iter = 31, f_k = 7130.713466528626\n",
      "iter = 32, f_k = 7116.3746661306595\n",
      "iter = 33, f_k = 7102.817258794328\n",
      "iter = 34, f_k = 7089.977673311976\n",
      "iter = 35, f_k = 7077.7997014138355\n",
      "iter = 36, f_k = 7066.23337056797\n",
      "iter = 37, f_k = 7055.23400208368\n",
      "iter = 38, f_k = 7044.761427140122\n",
      "iter = 39, f_k = 7034.779339652603\n",
      "iter = 40, f_k = 7025.254762354759\n",
      "iter = 41, f_k = 7016.15760753988\n",
      "iter = 42, f_k = 7007.46031644663\n",
      "iter = 43, f_k = 6999.137561775645\n",
      "iter = 44, f_k = 6991.166001699651\n",
      "iter = 45, f_k = 6983.524075121889\n",
      "iter = 46, f_k = 6976.191829895355\n",
      "iter = 47, f_k = 6969.1507768844795\n",
      "iter = 48, f_k = 6962.383765932129\n",
      "iter = 49, f_k = 6955.874876522972\n",
      "iter = 50, f_k = 6949.609322309852\n",
      "iter = 51, f_k = 6943.573366216478\n",
      "iter = 52, f_k = 6937.754243443889\n",
      "iter = 53, f_k = 6932.140091348456\n",
      "iter = 54, f_k = 6926.71988491003\n",
      "iter = 55, f_k = 6921.483376927447\n",
      "iter = 56, f_k = 6916.421042302604\n",
      "iter = 57, f_k = 6911.524025944268\n",
      "iter = 58, f_k = 6906.78409395554\n",
      "iter = 59, f_k = 6902.193587849766\n",
      "iter = 60, f_k = 6897.745381610513\n",
      "iter = 61, f_k = 6893.4328415255195\n",
      "iter = 62, f_k = 6889.249788362894\n",
      "iter = 63, f_k = 6885.190462170301\n",
      "iter = 64, f_k = 6881.24949006727\n",
      "iter = 65, f_k = 6877.421854792867\n",
      "iter = 66, f_k = 6873.702867000311\n",
      "iter = 67, f_k = 6870.088138874936\n",
      "iter = 68, f_k = 6866.573559899321\n",
      "iter = 69, f_k = 6863.15527450673\n",
      "iter = 70, f_k = 6859.829661521305\n",
      "iter = 71, f_k = 6856.59331527583\n",
      "iter = 72, f_k = 6853.443028313176\n",
      "iter = 73, f_k = 6850.375775525946\n",
      "iter = 74, f_k = 6847.3886996944375\n",
      "iter = 75, f_k = 6844.479098270584\n",
      "iter = 76, f_k = 6841.6444113329335\n",
      "iter = 77, f_k = 6838.882210587504\n",
      "iter = 78, f_k = 6836.190189462874\n",
      "iter = 79, f_k = 6833.566153795118\n",
      "iter = 80, f_k = 6831.008014040899\n",
      "iter = 81, f_k = 6828.513777323407\n",
      "iter = 82, f_k = 6826.081540698176\n",
      "iter = 83, f_k = 6823.709485007557\n",
      "iter = 84, f_k = 6821.395869222441\n",
      "iter = 85, f_k = 6819.139025282814\n",
      "iter = 86, f_k = 6816.937353407127\n",
      "iter = 87, f_k = 6814.789317818387\n",
      "iter = 88, f_k = 6812.6934428375\n",
      "iter = 89, f_k = 6810.648309311681\n",
      "iter = 90, f_k = 6808.652551346258\n",
      "iter = 91, f_k = 6806.704853304643\n",
      "iter = 92, f_k = 6804.803947052546\n",
      "iter = 93, f_k = 6802.948609421631\n",
      "iter = 94, f_k = 6801.137659870085\n",
      "iter = 95, f_k = 6799.369958319744\n",
      "iter = 96, f_k = 6797.64440315384\n",
      "iter = 97, f_k = 6795.959929373286\n",
      "iter = 98, f_k = 6794.315506843188\n",
      "iter = 99, f_k = 6792.710138694881\n",
      "iter = 100, f_k = 6791.1428597413105\n"
     ]
    }
   ],
   "source": [
    "aux_prob_method = 1\n",
    "xans_tensor3, farr_tensor3 = Tensor3_iter_hist(NumIter, TensorMaxIter, AuxMaxIter, w0, logistic_loss, grad_logistic_loss,\n",
    "            hess_logistic_loss, dot_tensor3_and_vector_vector_logistic_loss, tau, L3, eps, aux_prob_method, *args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13862.9436112 ,  11421.66335665,  10254.52009982,   9516.40449922,\n",
       "         9024.19160974,   8691.00855211,   8456.96422112,   8282.48930278,\n",
       "         8143.95084358,   8028.36642997,   7928.83386348,   7841.55450955,\n",
       "         7764.23246038,   7695.30588465,   7633.59822708,   7578.16129401,\n",
       "         7528.20135516,   7483.04053942,   7442.09371398,   7404.85281694,\n",
       "         7370.87530869,   7339.77518681,   7311.21572505,   7284.90341643,\n",
       "         7260.58276691,   7238.03172239,   7217.05757734,   7197.49328203,\n",
       "         7179.19410004,   7162.03459547,   7145.90592992,   7130.71346653,\n",
       "         7116.37466613,   7102.81725879,   7089.97767331,   7077.79970141,\n",
       "         7066.23337057,   7055.23400208,   7044.76142714,   7034.77933965,\n",
       "         7025.25476235,   7016.15760754,   7007.46031645,   6999.13756178,\n",
       "         6991.1660017 ,   6983.52407512,   6976.1918299 ,   6969.15077688,\n",
       "         6962.38376593,   6955.87487652,   6949.60932231,   6943.57336622,\n",
       "         6937.75424344,   6932.14009135,   6926.71988491,   6921.48337693,\n",
       "         6916.4210423 ,   6911.52402594,   6906.78409396,   6902.19358785,\n",
       "         6897.74538161,   6893.43284153,   6889.24978836,   6885.19046217,\n",
       "         6881.24949007,   6877.42185479,   6873.702867  ,   6870.08813887,\n",
       "         6866.5735599 ,   6863.15527451,   6859.82966152,   6856.59331528,\n",
       "         6853.44302831,   6850.37577553,   6847.38869969,   6844.47909827,\n",
       "         6841.64441133,   6838.88221059,   6836.19018946,   6833.5661538 ,\n",
       "         6831.00801404,   6828.51377732,   6826.0815407 ,   6823.70948501,\n",
       "         6821.39586922,   6819.13902528,   6816.93735341,   6814.78931782,\n",
       "         6812.69344284,   6810.64830931,   6808.65255135,   6806.7048533 ,\n",
       "         6804.80394705,   6802.94860942,   6801.13765987,   6799.36995832,\n",
       "         6797.64440315,   6795.95992937,   6794.31550684,   6792.71013869,\n",
       "         6791.14285974])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr_tensor3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('tensor3_result_log_regr.txt', farr_tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing autograd and formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.56662864e+03  -1.86405556e+03  -1.15640142e+03  -6.42997164e+02\n",
      "  -1.22648108e+03  -4.01957061e+02  -3.28178577e+03  -3.35611671e+03\n",
      "  -2.06313579e+03   8.35405535e+01   7.57500000e+02  -1.83500000e+02\n",
      "  -2.23450000e+03  -2.31750000e+03  -1.77500000e+02  -3.08500000e+02\n",
      "  -4.69000000e+02  -4.01500000e+02  -8.25000000e+01  -3.18000000e+02\n",
      "   0.00000000e+00   5.00000000e-01   4.00000000e+00  -9.90000000e+02\n",
      "  -1.36000000e+02   2.72000000e+02  -1.54000000e+02  -8.45000000e+01\n",
      "   0.00000000e+00   7.75000000e+01  -2.99000000e+02   2.90000000e+01\n",
      "  -1.00000000e+01  -1.00000000e+01  -8.00000000e+00  -1.18500000e+02\n",
      "  -2.29500000e+02  -5.20000000e+01   5.00000000e-01  -8.00000000e+00\n",
      "  -2.50000000e+00   1.50000000e+00   6.05500000e+02   1.51500000e+02\n",
      "  -6.90000000e+01  -9.00000000e+01  -1.24000000e+02   1.00000000e+00\n",
      "  -5.00000000e+01  -3.00000000e+00  -1.70000000e+01  -3.57000000e+02\n",
      "  -3.25500000e+02  -2.26500000e+02  -3.97800000e+03]\n",
      "[ -1.56662864e+03  -1.86405556e+03  -1.15640142e+03  -6.42997164e+02\n",
      "  -1.22648108e+03  -4.01957061e+02  -3.28178577e+03  -3.35611671e+03\n",
      "  -2.06313579e+03   8.35405535e+01   7.57500000e+02  -1.83500000e+02\n",
      "  -2.23450000e+03  -2.31750000e+03  -1.77500000e+02  -3.08500000e+02\n",
      "  -4.69000000e+02  -4.01500000e+02  -8.25000000e+01  -3.18000000e+02\n",
      "   0.00000000e+00   5.00000000e-01   4.00000000e+00  -9.90000000e+02\n",
      "  -1.36000000e+02   2.72000000e+02  -1.54000000e+02  -8.45000000e+01\n",
      "   0.00000000e+00   7.75000000e+01  -2.99000000e+02   2.90000000e+01\n",
      "  -1.00000000e+01  -1.00000000e+01  -8.00000000e+00  -1.18500000e+02\n",
      "  -2.29500000e+02  -5.20000000e+01   5.00000000e-01  -8.00000000e+00\n",
      "  -2.50000000e+00   1.50000000e+00   6.05500000e+02   1.51500000e+02\n",
      "  -6.90000000e+01  -9.00000000e+01  -1.24000000e+02   1.00000000e+00\n",
      "  -5.00000000e+01  -3.00000000e+00  -1.70000000e+01  -3.57000000e+02\n",
      "  -3.25500000e+02  -2.26500000e+02  -3.97800000e+03]\n"
     ]
    }
   ],
   "source": [
    "grad_logistic_loss_auto = grad(logistic_loss)\n",
    "hess_logistic_loss_auto = jacobian(grad_logistic_loss_auto)\n",
    "tensor3_logistic_loss_auto = jacobian(hess_logistic_loss_auto)\n",
    "\n",
    "print(grad_logistic_loss(w0, *args))\n",
    "print(grad_logistic_loss_auto(w0, *args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16029057e+03   9.29797380e+02   4.99013647e+02   3.77857408e+02\n",
      "   6.32462807e+02   7.37414754e+02   1.87871855e+03   1.93684404e+03\n",
      "   1.19865850e+03   8.24095873e+02   1.05332353e+03   9.24887368e+01\n",
      "   8.44931576e+02   2.34505449e+02   1.33599355e+01   4.61442049e+01\n",
      "   4.78331817e+01   7.11512087e+01   6.76350986e+00   4.23619403e+01\n",
      "   0.00000000e+00   1.30190000e-01   8.98824000e-01   1.39959510e+02\n",
      "   4.03670595e+01   1.32846554e+02   5.84209612e+01   7.14194745e+00\n",
      "   0.00000000e+00   4.11544535e+01   3.69115846e+01   3.07417492e+01\n",
      "   6.41845875e+00   2.81361940e+01   2.61630775e+00   5.56600773e+01\n",
      "   1.11824784e+02   3.97098530e+01   1.79339750e-01   6.98649375e+00\n",
      "   2.66520775e+00   9.81366000e-01   4.43180969e+02   1.89776510e+02\n",
      "   4.77526253e+01   1.05235488e+02   8.79214560e+01   3.10817850e+00\n",
      "   1.91253100e+01   1.93721875e+00   6.54752300e+00   1.35745613e+02\n",
      "   1.20880056e+02   9.26734525e+01   2.22524930e+03]\n",
      "[  1.16029057e+03   9.29797380e+02   4.99013647e+02   3.77857408e+02\n",
      "   6.32462807e+02   7.37414754e+02   1.87871855e+03   1.93684404e+03\n",
      "   1.19865850e+03   8.24095873e+02   1.05332353e+03   9.24887368e+01\n",
      "   8.44931576e+02   2.34505449e+02   1.33599355e+01   4.61442049e+01\n",
      "   4.78331817e+01   7.11512087e+01   6.76350986e+00   4.23619403e+01\n",
      "   0.00000000e+00   1.30190000e-01   8.98824000e-01   1.39959510e+02\n",
      "   4.03670595e+01   1.32846554e+02   5.84209612e+01   7.14194745e+00\n",
      "   0.00000000e+00   4.11544535e+01   3.69115846e+01   3.07417492e+01\n",
      "   6.41845875e+00   2.81361940e+01   2.61630775e+00   5.56600773e+01\n",
      "   1.11824784e+02   3.97098530e+01   1.79339750e-01   6.98649375e+00\n",
      "   2.66520775e+00   9.81366000e-01   4.43180969e+02   1.89776510e+02\n",
      "   4.77526253e+01   1.05235488e+02   8.79214560e+01   3.10817850e+00\n",
      "   1.91253100e+01   1.93721875e+00   6.54752300e+00   1.35745613e+02\n",
      "   1.20880056e+02   9.26734525e+01   2.22524930e+03]\n"
     ]
    }
   ],
   "source": [
    "print(hess_logistic_loss(w0, *args)[0])\n",
    "print(hess_logistic_loss_auto(w0, *args)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.75127447e-10  -5.76882441e-10  -7.95231482e-10  -6.87292495e-10]\n",
      "[ -2.89789589e-10  -5.55430045e-10  -8.21070502e-10  -6.76175707e-10]\n"
     ]
    }
   ],
   "source": [
    "train_extended_object = np.array([[1.2, 2.3, 3.4, 2.8], [19.7, 5.5, 6.6, 7.7]])\n",
    "train_ans = np.array([1, -1])\n",
    "w0 = np.array([5.6, 1.3, 5, 1.2])\n",
    "h = np.array([1.1, 1.9, 12.4, 1.7])\n",
    "args = (train_extended_object, train_ans)\n",
    "tensor_3_auto = tensor3_logistic_loss_auto(w0, *args)\n",
    "print(np.dot(np.dot(tensor_3_auto, h), h))\n",
    "print(dot_tensor3_and_vector_vector_logistic_loss(w0, h, *args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02729094 -0.01105561 -0.00280546 -0.01248178 -0.01632065 -0.00468862\n",
      " -0.06452449 -0.06833928 -0.0437575  -0.06478162 -0.07433052  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.07433052  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.07433052]\n",
      "[-0.02729094 -0.01105561 -0.00280546 -0.01248178 -0.01632065 -0.00468862\n",
      " -0.06452449 -0.06833928 -0.0437575  -0.06478162 -0.07433052  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.07433052  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.07433052]\n"
     ]
    }
   ],
   "source": [
    "train_const_feature = np.array([np.array([1]) for i in range(train_object_size)]) #добавим фиктивный признак\n",
    "train_extended_object = copy.deepcopy(train_object)\n",
    "train_extended_object = np.column_stack((train_extended_object, train_const_feature[:, 0]))\n",
    "args = (train_extended_object, train_ans)\n",
    "\n",
    "features_num = features + 1\n",
    "w0 = np.ones(features_num)\n",
    "h = np.ones(features_num)\n",
    "tensor_3_auto = tensor3_logistic_loss_auto(w0, *args)\n",
    "print(np.dot(np.dot(tensor_3_auto, h), h))\n",
    "print(dot_tensor3_and_vector_vector_logistic_loss(w0, h, *args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
